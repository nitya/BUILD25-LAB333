{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e69c292",
   "metadata": {},
   "source": [
    "# Prompt Engineering Principles for OpenAI Reasoning Models (o4-mini & o1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55982c4",
   "metadata": {},
   "source": [
    "\n",
    "## Principles Overview\n",
    "\n",
    "| Prompting Principle | What it means for **o-series reasoning models (o4-mini & o1)** |\n",
    "|---|---|\n",
    "| **1 â€” Use a `developer` message (not system) and give a clear role** | The newest reasoning models treat a *system* message as a *developer* message; mixing both is discouraged. Put the highâ€‘level instruction in a single `role:\"developer\"` message, e.g., â€œYou are an expert tax lawyerâ€¦â€. |\n",
    "| **2 â€” Keep prompts simple & direct â€” donâ€™t force chainâ€‘ofâ€‘thought** | Reasoning models already â€œthinkâ€ internally. Overâ€‘specifying (â€œthink stepâ€‘byâ€‘stepâ€¦ explain every thoughtâ€) wastes tokens and can leak private reasoning. Ask only for the final answer unless you truly need a public explanation. |\n",
    "| **3 â€” Use explicit delimiters to structure input** | Wrap long passages, code, or multiâ€‘part instructions in clear fences (` ````, `<doc>â€¦</doc>`, Markdown headings). Delimiters help the model parse sections correctly and reduce misâ€‘interpretation. |\n",
    "| **4 â€” Supply *only* the relevant context** | With 200â€¯kâ€‘token windows you can paste huge docsâ€”but you *shouldnâ€™t*. Include the minimal excerpts the task needs so the model focuses on the right evidence and stays concise. |\n",
    "| **5 â€” Decompose or iterate instead of one giant ask (try zeroâ€‘shot first)** | Start with a straightforward version, examine the answer, then refine or break the workflow into numbered subâ€‘tasks. This lets the model reason deeply on each step and saves tokens. |\n",
    "| **6 â€” Specify output format & boundaries** | Tell the model exactly *how* to answer (JSON schema, Markdown table, â€œâ‰¤â€¯120â€¯wordsâ€, etc.). Reasoning models follow detailed format guards well and will keep their lengthy analysis inside your boundaries. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91719bb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Define Chat Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4719d061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment looks good: All variables are set.\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "if not all(os.getenv(var) for var in [\"AZURE_OPENAI_KEY\", \"AZURE_OPENAI_ENDPOINT\", \"AZURE_OPENAI_API_VERSION\", \"REASONING_NEW\"]): \n",
    "    raise ValueError(\"âŒ Missing one or more required env vars: Check .env.\")\n",
    "\n",
    "print(\"âœ… Environment looks good: All variables are set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56fc0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print stats\n",
    "def print_token_and_filter_info(r):\n",
    "\n",
    "    # Print token usage\n",
    "    print(\".........................\")\n",
    "    print(\"Token Costs:\")\n",
    "    print(f\"Total Tokens: {r.usage.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {r.usage.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {r.usage.completion_tokens}\")\n",
    "    print(f\"Reasoning Tokens: {r.usage.completion_tokens_details.reasoning_tokens}\")\n",
    "    print(f\"Output Tokens: {r.usage.total_tokens - r.usage.completion_tokens_details.reasoning_tokens}\")\n",
    "    print(\".........................\")\n",
    "\n",
    "    # Print content filter results\n",
    "    '''\n",
    "    print(\"Content Filter Results:\")\n",
    "    filter_results = getattr(r.choices[0], \"content_filter_results\", None)\n",
    "    if filter_results is not None:\n",
    "        for k, v in filter_results.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No content filter results available.\")\n",
    "    print(\".........................\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default chat completion with developer persona\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "def chat(model: str = os.getenv(\"REASONING_NEW\"), persona: str=\"You are a friendly and helpful assistant.\", query: str = \"Hello!\", **kwargs):\n",
    "    r = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\":\"developer\",\"content\":persona},\n",
    "            {\"role\":\"user\",\"content\":query}\n",
    "        ],      \n",
    "        **kwargs\n",
    "    )\n",
    "    print(f\"ğŸ—£ï¸ {model} returned:\")\n",
    "    print(r.choices[0].message.content)\n",
    "\n",
    "    # pretty print stats\n",
    "    print_token_and_filter_info(r)\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets you share history of messages and customize the persona\n",
    "def chat_with_history(messages: list, model: str=os.getenv(\"REASONING_NEW\"), query: str = \"Hello!\", **kwargs):\n",
    "    messages = messages + [{\"role\": \"user\", \"content\": query}]\n",
    "    r = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs\n",
    "    )\n",
    "    print(f\"ğŸ—£ï¸ {model} returned:\")\n",
    "    print(r.choices[0].message.content)\n",
    "    print_token_and_filter_info(r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3db9b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Principle 1: Developer Messages\n",
    "\n",
    "- Use a developer message (not system) - and don't mix them\n",
    "- Define a clear task scope or persona (Act as XYZ)\n",
    "- Specify an output format if it make sense (Give me a list of items)\n",
    "- Set boundaries (Use simple language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7a4c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—£ï¸ o4-mini returned:\n",
      "Hi, friends! Today weâ€™re going to learn about â€œreasoning models.â€ That sounds like a big phrase, but itâ€™s really just talking about ways our brain figures things out. Think of it like different tools or helpers we can use when we want to solve a problem or answer a question. Letâ€™s look at three simple helpers:\n",
      "\n",
      "1. â€œStepâ€‘byâ€‘Stepâ€ Helper  \n",
      "   â€¢ Imagine youâ€™re making a peanutâ€‘butter sandwich. You follow steps:  \n",
      "     1) Take two slices of bread.  \n",
      "     2) Spread peanut butter on one slice.  \n",
      "     3) Put the slices together.  \n",
      "   â€¢ When we solve a question, we can do the sameâ€”go one small step at a time until we get our answer!\n",
      "\n",
      "2. â€œPattern Detectiveâ€ Helper  \n",
      "   â€¢ A detective looks for clues and patterns. For example, you see: 2, 4, 6, 8, __.  \n",
      "   â€¢ You notice â€œeach number goes up by 2.â€ So the next number is 10!  \n",
      "   â€¢ Our brain can be a pattern detective, too, spotting whatâ€™s the same or what comes next.\n",
      "\n",
      "3. â€œTreasureâ€‘Mapâ€ Helper (Working Backward)  \n",
      "   â€¢ Pretend you want to find buried treasure. You know the X on the map, and you work backward from X to the starting tree.  \n",
      "   â€¢ In thinking, sometimes you start with the answer you want, then figure out each clue in reverse until you begin.\n",
      "\n",
      "Why use these helpers?  \n",
      "- They keep our thoughts neat and easy to follow.  \n",
      "- They help us check each little piece, so we donâ€™t get lost.  \n",
      "- They make tricky problems feel simple, like a fun game!\n",
      "\n",
      "Letâ€™s try one together:  \n",
      "Question: â€œI have 3 red apples and 2 green apples. How many apples do I have in all?â€  \n",
      "\n",
      "â€¢ Stepâ€‘byâ€‘Step Helper:  \n",
      "  1) Count the red apples: 3.  \n",
      "  2) Count the green apples: 2.  \n",
      "  3) Add them: 3 + 2 = 5.  \n",
      "Answer: 5 apples!\n",
      "\n",
      "Isnâ€™t that fun? Anytime you have a questionâ€”math, reading, or how to clean up your toysâ€”you can pick one of these helpers (or even all three!) and watch your brain light up with the answers. Great job learning about reasoning models today!\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 1320\n",
      "Prompt Tokens: 30\n",
      "Completion Tokens: 1290\n",
      "Reasoning Tokens: 768\n",
      "Output Tokens: 552\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "protected_material_code: {'filtered': False, 'detected': False}\n",
      "protected_material_text: {'filtered': False, 'detected': False}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# BAD PROMPT\n",
    "# Uses a system message as well as a developer message.\n",
    "# Sets no boundaries or clarity for response\n",
    "messages=[{\"role\":\"system\",\"content\":\"You are ChatGPT\"}, {\"role\":\"developer\",\"content\":\"You are a 1st grade teacher.\"}]\n",
    "query=\"Explain reasoning models\"\n",
    "test = chat_with_history(messages=messages, query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9cbf853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—£ï¸ o4-mini returned:\n",
      "Reasoning models are like treasure maps in your head:  \n",
      "They give you steps to follow, from clue A to clue Z.  \n",
      "Step by step, you never stray,  \n",
      "Finding answers along the way!\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 354\n",
      "Prompt Tokens: 37\n",
      "Completion Tokens: 317\n",
      "Reasoning Tokens: 256\n",
      "Output Tokens: 98\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# GOOD PROMPT\n",
    "# Using the system role\n",
    "# Role description is very vague\n",
    "messages=[{\"role\": \"developer\", \"content\": \"You are a 1st grade teacher. Explain concepts with analogies and rhymes. Keep answer simple ans short.\"}]\n",
    "query=\"Explain reasoning models\"\n",
    "test = chat_with_history(messages=messages, query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9fdb3",
   "metadata": {},
   "source": [
    "### 2. Simple & Direct. No CoT\n",
    "\n",
    "- Keep it simple. \n",
    "- Give high-level guidance and let the model figure it out. \n",
    "- Don't offer irrelevant details - less is more\n",
    "- Don't ask it to think step by step - it internalizes chain of thought already\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Prompt** | \"Explain step by step who is lying and who is telling the truth. Show all your reasoning in detail so I can follow your chain of thought\" | \"A says â€˜B is a liar.â€™ B says â€˜C is a knight.â€™ C says nothing. Who is telling the truth?\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f955a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—£ï¸ o4-mini returned:\n",
      "Letâ€™s call a person a â€œKnightâ€ if they always tell the truth, and a â€œLiarâ€ if they always lie.  We have three statements:\n",
      "\n",
      " 1. A says â€œB is a liar.â€  \n",
      " 2. B says â€œC is a knight.â€  \n",
      " 3. C says nothing.  \n",
      "\n",
      "We ask: which assignments of Knight (K) and Liar (L) to A, B, and C are logically consistent?\n",
      "\n",
      "---\n",
      "\n",
      "Step 1: Translate Aâ€™s statement.\n",
      "\n",
      "â€“ If A is a Knight, then his statement â€œB is a liarâ€ is true â‡’ B is L.  \n",
      "â€“ If A is a Liar, then his statement is false â‡’ B is not a liar â‡’ B is K.  \n",
      "\n",
      "So from A we get two branches:\n",
      "  \n",
      "  Branch 1: A = K â‡’ B = L  \n",
      "  Branch 2: A = L â‡’ B = K  \n",
      "\n",
      "---\n",
      "\n",
      "Step 2: In each branch, analyze Bâ€™s statement â€œC is a knight.â€\n",
      "\n",
      "Branch 1 (A=K, B=L):  \n",
      "â€“ B is a Liar, so his statement is false.  \n",
      "â€“ â€œC is a knightâ€ is false â‡’ C = L.\n",
      "\n",
      "Conclusion of Branch 1:  \n",
      "  A = K, B = L, C = L  \n",
      "  (Check: Aâ€™s true statement B=L is good. B, being Liar, lies about C, so C really is L.  C says nothingâ€”no contradiction.)\n",
      "\n",
      "Branch 2 (A=L, B=K):  \n",
      "â€“ B is a Knight, so his statement is true.  \n",
      "â€“ â€œC is a knightâ€ is true â‡’ C = K.\n",
      "\n",
      "Conclusion of Branch 2:  \n",
      "  A = L, B = K, C = K  \n",
      "  (Check: A, being Liar, lies when he says â€œB is a liar,â€ since in fact B=K.  B truthfully says C=K.  C silentâ€”no contradiction.)\n",
      "\n",
      "---\n",
      "\n",
      "Step 3: List the possible solutions.\n",
      "\n",
      "We have found two fully consistent assignments:\n",
      "\n",
      " 1. A = Knight, B = Liar,  C = Liar  \n",
      " 2. A = Liar,  B = Knight, C = Knight  \n",
      "\n",
      "Neither leads to a logical contradiction, and with only the given statements there is no further condition (like â€œexactly one knight,â€ etc.) to pick just one.  \n",
      "\n",
      "Therefore the puzzle as stated admits exactly these two solutions.\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 1355\n",
      "Prompt Tokens: 66\n",
      "Completion Tokens: 1289\n",
      "Reasoning Tokens: 768\n",
      "Output Tokens: 587\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "protected_material_code: {'filtered': False, 'detected': False}\n",
      "protected_material_text: {'filtered': False, 'detected': False}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# BAD PROMPT\n",
    "# Overly verbose and forces chain-of-thought reasoning\n",
    "bad_query = (\n",
    "    \"A says â€˜B is a liar.â€™ B says â€˜C is a knight.â€™ C says nothing. Explain step by step who is lying and who is telling the truth. Show all your reasoning in detail so I can follow your chain of thought\"\n",
    ")\n",
    "test = chat(query=bad_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—£ï¸ o4-mini returned:\n",
      "Let A,Â B,Â C be â€œknightsâ€ (always tell the truth) or â€œliarsâ€ (always lie).  \n",
      "A says â€œB is a liar.â€  \n",
      "B says â€œC is a knight.â€  \n",
      "C says nothing.  \n",
      "\n",
      "Call A,Â B,Â C = T (knight) or F (liar).  Then truthâ€value of  \n",
      "  â€¢ Aâ€™s statement (â€œB is a liarâ€) is [B = F].  \n",
      "  â€¢ Bâ€™s statement (â€œC is a knightâ€) is [C = T].  \n",
      "\n",
      "We must have  \n",
      "  â€“ If A = T, then B = F.  \n",
      "      But then B = F â‡’ Bâ€™s statement is false â‡’ C = F.  \n",
      "      â‡’ (A,B,C) = (T,F,F) is selfâ€consistent.  \n",
      "  â€“ If A = F, then Aâ€™s statement is false â‡’ B = T.  \n",
      "      Then B = T â‡’ Bâ€™s statement is true â‡’ C = T.  \n",
      "      â‡’ (A,B,C) = (F,T,T) is also selfâ€consistent.  \n",
      "\n",
      "No other assignment works.  Hence there are exactly two solutions:  \n",
      " 1) A = knight, B = liar, C = liar  \n",
      " 2) A = liar,  B = knight, C = knight  \n",
      "\n",
      "In particular exactly one of A or B is telling the truth, and C turns out to be the same type as B.\n",
      ".........................\n",
      "Token Costs:\n",
      "Total Tokens: 2022\n",
      "Prompt Tokens: 44\n",
      "Completion Tokens: 1978\n",
      "Reasoning Tokens: 1664\n",
      "Output Tokens: 358\n",
      ".........................\n",
      "Content Filter Results:\n",
      "hate: {'filtered': False, 'severity': 'safe'}\n",
      "protected_material_code: {'filtered': False, 'detected': False}\n",
      "protected_material_text: {'filtered': False, 'detected': False}\n",
      "self_harm: {'filtered': False, 'severity': 'safe'}\n",
      "sexual: {'filtered': False, 'severity': 'safe'}\n",
      "violence: {'filtered': False, 'severity': 'safe'}\n",
      ".........................\n"
     ]
    }
   ],
   "source": [
    "# GOOD PROMPT\n",
    "# Simple, direct, and does not force chain-of-thought reasoning\n",
    "good_query = \"A says â€˜B is a liar.â€™ B says â€˜C is a knight.â€™ C says nothing. Who is telling the truth?\"\n",
    "test = chat(query=good_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810c09b",
   "metadata": {},
   "source": [
    "### 3â€‚Delimiters for structure\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pairâ€¯A** | â€œSummarize this plus write codeâ€ (pastes code & prose unâ€‘separated) | â€œSummarize the prose, then improve the code in **Sectionâ€¯2** below.\\n### SectionÂ 1 â€“Â Prose\\n```text\\n...\\n```\\n### SectionÂ 2 â€“Â Code\\n```python\\n...\\n```â€ |\n",
    "| **Pairâ€¯B** | â€œFix errors in my SQL:â€ + random HTML fragment mixed in | â€œBetween `<sql>` tags is my query; return only the corrected query.\\n<sql>\\nSELECT * FROM orders o JOIN customers c ON id;\\n</sql>â€ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a57a81",
   "metadata": {
    "vscode": {
     "languageId": "xml"
    }
   },
   "outputs": [],
   "source": [
    "# GOOD PROMPT using XML delimiters for structure\n",
    "query = \"\"\"\n",
    "Summarize the recipe below in 3 bullet points, focusing on key ingredients and preparation steps.\n",
    "<recipe>\n",
    "<name>Mango Margaritas</name>\n",
    "<ingredients>2 cups mango, 1 cup tequila, 1/2 cup lime juice, 1/4 cup triple sec</ingredients>\n",
    "<instructions>Blend mango, tequila, lime juice, and triple sec until smooth. Serve over ice.</instructions>\n",
    "<serving>4</serving>\n",
    "<calories>200</calories>\n",
    "<prep_time>10 minutes</prep_time>\n",
    "<total_time>10 minutes</total_time>\n",
    "<notes>Refreshing summer drink, perfect for parties!</notes>\n",
    "<tips>Use fresh mango for best flavor.</tips>\n",
    "</recipe>\n",
    "\"\"\"\n",
    "test = chat(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3951dc20",
   "metadata": {},
   "source": [
    "### 4â€‚Relevant context only\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pairâ€¯A** | â€œSummarize ACMEâ€™s entire 300â€‘page 10â€‘K (pasted below) in 3 bullets.â€ | â€œSummarize **RiskÂ Factors** (ppâ€¯12â€‘15) from ACMEâ€™s 2024Â 10â€‘K into 3 bullets.â€ |\n",
    "| **Pairâ€¯B** | â€œBased on these 10 articles (pasted), who won the case?â€ | â€œUsing the quoted judgment excerpt below, identify which party (SmithÂ orÂ Jones) prevailed.\\n```<judgment>â€¦</judgment>```â€ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f130c5ac",
   "metadata": {},
   "source": [
    "### 5â€‚Decompose / iterate\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pairâ€¯A** | â€œWrite a 40â€‘page business plan including market, finances, HR, legal.â€ | â€œStepÂ 1 â€” outline sections & bullet points. *Wait.*\\nStepÂ 2 â€” expand the **Market Analysis** section to ~500Â words.â€ |\n",
    "| **Pairâ€¯B** | â€œTranslate, summarize and turn into slidesâ€”in one go.â€ | â€œ(a) Translate the article to English.Â Â (b) Summarize it in 5 bullets.Â Â (c) Provide slide headlines.â€ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685a3f4",
   "metadata": {},
   "source": [
    "### 6â€‚Output format & boundaries\n",
    "\n",
    "\n",
    "| | Bad Prompt | Good Prompt |\n",
    "|---|---|---|\n",
    "| **Pairâ€¯A** | â€œTell me key poll data for EU elections.â€ | â€œReturn JSON array `{country, pollster, sample_size, lead_pct}` for Germany, France, Spain (2024Â polls only).â€ |\n",
    "| **Pairâ€¯B** | â€œExplain transformers.â€ | â€œExplain transformer architecture in **exactly five bullet points of â‰¤â€¯15â€¯words each**, no code blocks.â€ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae3c97",
   "metadata": {},
   "source": [
    "\n",
    "### References\n",
    "\n",
    "- OpenAIÂ ReasoningÂ Guide â€“ <https://platform.openai.com/docs/guides/reasoning>  \n",
    "- OpenAIÂ Reasoning Best Practices â€“ <https://platform.openai.com/docs/guides/reasoning-best-practices>  \n",
    "- Azure TechCommunity Blog: *Prompt Engineering for OpenAIâ€™s O1 and O3â€‘mini ReasoningÂ Models* â€“ <https://techcommunity.microsoft.com/blog/azure-ai-services-blog/prompt-engineering-for-openai%E2%80%99s-o1-and-o3-mini-reasoning-models/4374010>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

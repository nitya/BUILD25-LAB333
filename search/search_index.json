{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Lab 333: Evaluating Reasoning Models for your Generative AI Solutions","text":"<p>Duration: 75 minutes</p> <p>This workshop gives you a hand-on introduction to reasoning models from core concepts to practical usage in Azure AI Foundry. It has been created for use in both instructor-led (in-venue) and self-paced (at-home) usage. </p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>This workshop is designed to jumpstart your journey into understanding, and working with, reasoning models in generative AI. By the end of the journey you should be able to:</p> <ol> <li>Explain what reasoning models are, and how they work</li> <li>Deploy and explore reasoning models using Azure AI Foundry and GitHub</li> <li>Build intution for usage with low-code and code-first approaches</li> <li>Gain insights into patterns and practices for practical applications</li> </ol>"},{"location":"#pre-requisites","title":"Pre-Requisites","text":"<ol> <li>GitHub Account - you can signup for free.</li> <li>Azure Account - you can signup for free.</li> <li>Familiarity with Python programming &amp; Jupyter notebooks</li> <li>Familiarity with VS Code editors &amp; commandline usage</li> <li>Familiarity with Generative AI applications and concepts</li> </ol> <p>IN-VENUE ATTENDEES WILL GET A PRE-PROVISIONED AZURE SUBSCRIPTION</p>"},{"location":"#lab-outline","title":"Lab Outline","text":"<p>The content is structured in two parts - a core \"Workshop\" track with a bonus \"Homework\" track. The workshop track can be completed within 75 mins and is structured as four sections:</p> <ol> <li>Setup: Create an Azure AI Foundry project with model deployments. Validate your setup.</li> <li>Introduction: Understand how reasoning models differ from general-purpose large language models. Build intuition for the tradeoffs made when choosing between them.</li> <li>Ideation: Start exploring model options in the Azure AI Foundry playground without code. Build intuition for reasoning effort and the impact it has on model performance.</li> <li>Inference: Move to a development enviroment for code-first exploration of scenarios. Build intuition for good and bad use cases, and best practices for prompting.</li> </ol> <p>The homework track provides bonus content to help you continue the journey at your own pace. It will be updated over time - for now, it has one section:</p> <ol> <li>Implementation: Think about how these learnings translate to building a real-world application. Build intuition for patterns and practices for efficient usage.</li> </ol>"},{"location":"#keep-learning","title":"Keep Learning","text":"<p>This workshop is designed to be a living resource to continue your learning journey.</p> <ol> <li>Walk away with a sandbox (and notebooks) that you can use, to experiment with application ideas, or to explore advanced reasoning concepts.</li> <li>Star or watch the repo for updates. We'll continue to add new labs and scenarios to keep pace with the fast-growing set of models and capabilities for reasoning.</li> </ol>"},{"location":"#questions-or-comments","title":"Questions or Comments?","text":"<ol> <li>File an issue. We welcome feedback on ways to improve the workshop for future learners.</li> <li>Join the Azure AI Foundry Discord. Meet Azure AI community members and share insights.</li> <li>Visit the Azure AI Foundry Developer Forum. Get the latest updates on Azure AI Foundry.</li> </ol>"},{"location":"01-Workshop/00-Setup/00-self-guided/","title":"Lab 1: Setup AI Project","text":"<p>BY THE END OF THIS LAB YOU SHOULD HAVE</p> <ol> <li>A personal copy of the workshop repo in your GitHub profile</li> <li>A pre-configured development environment in GitHub Codespaces</li> <li>An provsisoned Azure AI project in Azure AI Foundry</li> </ol>"},{"location":"01-Workshop/00-Setup/00-self-guided/#1-launch-github-codespaces","title":"1. Launch GitHub Codespaces","text":"<p>This repository is configured with a <code>devcontainer.json</code> that can be activated in GitHub Codespaces (cloud) or Docker Desktop (local device) to give you a prebuild development environment with least effort. We recommend using GitHub Codespaces for this lab.</p> <ol> <li>Log into your personal GitHub profile</li> <li>Fork this repo to get a personal copy in your profile.</li> <li>Open the forked repo in your browser, click on \"Code\" dropdown</li> <li>Select the <code>Codespaces</code> tab and create a new codespace.</li> </ol> <p>This will open a new tab with a VS Code editor in the browser. Wait till the editor is fully loaded and the Visual Studio Code terminal is active.</p>"},{"location":"01-Workshop/00-Setup/00-self-guided/#2-verify-azure-cli-exists","title":"2. Verify Azure CLI Exists","text":"<ol> <li> <p>The <code>devcontainer</code> should have the Azure CLI pre-installed. Verify this now: </p> <pre><code>az version\n</code></pre> </li> <li> <p>You should see something like: </p> <pre><code>{\n\"azure-cli\": \"2.71.0\",\n\"azure-cli-core\": \"2.71.0\",\n\"azure-cli-telemetry\": \"1.1.0\",\n\"extensions\": {}\n}\n</code></pre> </li> </ol>"},{"location":"01-Workshop/00-Setup/00-self-guided/#3-authenticate-with-azure","title":"3. Authenticate with Azure","text":"<ol> <li> <p>Use the Azure CLI to log into your Azure account using this command.</p> <pre><code>az login --use-device-code\n</code></pre> </li> <li> <p>This starts a browser-based authentication flow. Follow the instructions in the terminal to complete the login process. Then return to the VS Code terminal. You are now connected to your Azure backend from the codespace.</p> </li> </ol> <p>IN-VENUE LAB ATTENDEES: You can skip the deploy step and go directly to Validate Setup:</p> <ol> <li>In-venue attendees - you get a pre-provisioned Azure subscription by default.</li> <li>At-home learners - you can provision these resources using your own Azure subscription</li> </ol>"},{"location":"01-Workshop/00-Setup/00-self-guided/#4-deploy-with-azure-cli","title":"4. Deploy with Azure CLI","text":"<ol> <li> <p>Next, use the Azure CLI to create a new resource group for the lab. This sets up a resource group called <code>Lab333</code> in the <code>EastUS 2</code> region.</p> <pre><code>az group create --name Lab333 --location 'EastUS 2'\n</code></pre> </li> <li> <p>You should see something like this. Your resource group is ready to be provisioned.</p> <pre><code>{\n    \"id\": \"/subscriptions/XXXXXX/resourceGroups/Lab333\",\n    \"location\": \"eastus2\",\n    \"managedBy\": null,\n    \"name\": \"Lab333\",\n    \"properties\": {\n        \"provisioningState\": \"Succeeded\"\n    },\n    \"tags\": null,\n    \"type\": \"Microsoft.Resources/resourceGroups\"\n}\n</code></pre> </li> <li> <p>Now, use this command to deploy the defined template to that resource group. </p> <pre><code>az deployment group create --name reasoning-template --resource-group Lab333 --template-file infra/template.json\n</code></pre> </li> <li> <p>You will be prompted for a uniqueSuffix - provide a random 4-digit number. This will reduce naming conflicts with soft-deleted resources from previous runs, that aren't yet purged.</p> </li> <li> <p>The script takes a few minutes to run. Next, we'll validate the setup.</p> </li> </ol>"},{"location":"01-Workshop/00-Setup/01-explore-project/","title":"Lab 2: Validate Setup","text":"<p>BY THE END OF THIS LAB YOU SHOULD HAVE</p> <ol> <li>Accessed Azure Portal and verified resources were provisioned</li> <li>Accessed Azure AI Foundry portal and verified models were deployed</li> <li>Activated the Azure AI Foundry playground for future use</li> </ol>"},{"location":"01-Workshop/00-Setup/01-explore-project/#1-log-into-azure-portal","title":"1. Log into Azure Portal","text":"<p>The Azure Portal is the web-based interface for managing Azure resources. It provides a unified view of all your Azure resources, including those provisioned by Azure AI Foundry. </p> <p>To log into the Azure portal, you will need an Azure subscription. Pick a tab below.</p> In-Venue AttendeesAt-Home Attendees <p>You will be using the Azure subscription provided by the Skillable lab environment.</p> <ol> <li>Visit this Azure Portal link in a new browser tab.   </li> <li>You will be prompted to log in.</li> <li>Switch to the browser tab with the Skillable VM<ul> <li>Look for the Azure Credentials in the instructions pane</li> <li>Copy the <code>Username</code>. Switch to the Azure Portal tab and paste it.</li> <li>Switch to Skillable tab. Copy the <code>Password</code> for the credential.</li> <li>Switch to Azure Portal tab. Paste it and complete auth flow. </li> </ul> </li> </ol> <p>You will be using your own Azure subscription.</p> <ol> <li>Visit this Azure Portal link in a new browser tab.</li> <li>You will be prompted to log in.</li> <li>Enter your own Azure subscription credentials (username, password)</li> <li>Complete the auth flow.</li> </ol> <p>Congratulations: You are logged into the Azure Portal</p>"},{"location":"01-Workshop/00-Setup/01-explore-project/#2-check-azure-resources","title":"2. Check Azure Resources","text":"<p>Let's verify that the resource group was provisioned correctly. By default, the link about should take you to a page listing all resource groups.</p> <ol> <li>You should the lab resource group listed (Lab333)</li> <li> <p>Click to view its details page - it should look similar to this:</p> <p></p> </li> <li> <p>Check that you have these resources provisioned</p> <ul> <li> Azure AI hub resource </li> <li> Azure AI project resource</li> <li> Azure AI services resource</li> <li> Key vault</li> <li> Storage account</li> </ul> </li> </ol>"},{"location":"01-Workshop/00-Setup/01-explore-project/#3-view-azure-ai-project","title":"3. View Azure AI Project","text":"<p>The Azure AI Foundry portal is a specialized interface for working with Azure AI Foundry projects. It provides a UI-based (low-code) way to interact with your Azure AI applications. </p> <p>Let's take a quick tour of the portal to get familiar with features.</p> <ol> <li> <p>Navigate to this Azure AI Foundry portal link in a new browser tab.</p> <ul> <li>The landing page lists your active Azure AI Foundry projects.</li> <li> <p>You should see at least one project listed now, as shown.</p> <p></p> </li> </ul> </li> <li> <p>Click the listed project to visit the project Overview page.</p> <ul> <li>This page gives you all configuration information needed for code-first access.</li> <li> <p>The sidebar also shows you UI-based tools &amp; services for working with the project.</p> <p></p> </li> </ul> </li> </ol>"},{"location":"01-Workshop/00-Setup/01-explore-project/#4-view-model-deployments","title":"4. View Model Deployments","text":"<p>AI Models are the core engines of your Azure AI Foundry project. Let's verify that all required models were provisioned correctly.</p> <ol> <li> <p>Click the Models + endpoints item in the sidebar</p> <ul> <li>This model deployments page lists currently deployed models for the project.</li> <li>Check that you have these three models deployed, as shown.<ul> <li> General Purpose (popular) - <code>gpt-4o-mini</code></li> <li> Reasoning (old, capable) - <code>o1</code></li> <li> Reasoning (new, fast, cheap) - <code>o4-mini</code></li> </ul> </li> </ul> <p></p> </li> <li> <p>Next, let's explore the model details page to learn what it provides.</p> <ul> <li>Click on one of the models in the list - e.g., click <code>o4-mini</code></li> <li>You see the details page below (with code samples &amp; config info)</li> <li>You see a metrics tab - this has stats on token usage, request rate etc.</li> <li> <p>Click the link under Model name to visit the model card.</p> <p></p> </li> </ul> </li> <li> <p>Next, let's locate the model card for this model.</p> <ul> <li>Click the <code>o4-mini</code> link under Model name to get the model card (below)</li> <li>This gives details on the model capabilities, pricing, and performance.</li> <li>We're using Global Standard model deployments (pay-as-you-go pricing)</li> <li> <p>Scan the variants to see how this compares to its peer reasoning models.</p> <p></p> </li> </ul> </li> </ol> <p>The Azure AI Foundry portal is invaluable during the Ideation phase of our project.</p> <p>You can explore the model catalog, assess models with benchmarks, and deploy a select few for closer inspection before making the final selection for development.</p>"},{"location":"01-Workshop/00-Setup/01-explore-project/#5-explore-the-playground","title":"5. Explore The Playground","text":"<p>The Model Playground is a low-code interface for exploring and testing models. Quickly prototype ideas, test prompts, and evaluate model performance - all without writing code.</p> <ol> <li> <p>Click on the Playgrounds option in the menu sidebar to get started</p> <ul> <li> <p>Click \"Try the Chat playground\" (this is a one-time activation)</p> <p></p> </li> <li> <p>You can then select any deployed model for interactive exploration</p> <p></p> </li> </ul> </li> <li> <p>We have 2 reasoning models and 1 general-purpose model available for testing.</p> <ul> <li>Let's explore the <code>gpt-4o-mini</code> model (we'll use reasoning models later)</li> <li> <p>Select a default prompt - e.g., I selected the Poetry generation prompt</p> <pre><code>Compose a poem about the beauty of nature in autumn\n</code></pre> </li> <li> <p>Click the arrow - note the response and the token usage (below the prompt box)</p> </li> <li> <p>Expand the Parameters section - vary settings to assess response quality</p> <p></p> </li> </ul> </li> </ol> <p>HOMEWORK: Things you can try on your own later</p> <ul> <li>Vary Max Response &amp; retry prompt - how did the response change?</li> <li>Change the model &amp; retry prompt - how do reasoning models differ?</li> <li>Observe impact on response quality, latency &amp; token cost in each case</li> </ul> <p>You just saw how easy it is to experiment with models and ideate in the Playground. This is how you build intuition for how a model works, and how to make it work effectively.</p>"},{"location":"01-Workshop/00-Setup/01-explore-project/#6-configure-local-environment","title":"6. Configure Local Environment","text":"<p>Ideation is good for assessing models for your project. But once you've made a decision, you need to move to a code-first approach for development. Let's set ourselves up now so we can explore code-first development later.</p> <ol> <li>Open the VS Code Terminal. Make sure you are at the root folder</li> <li> <p>Copy the <code>.env.sample</code> file to a <code>.env</code></p> <pre><code>cp .env.sample .env\n</code></pre> </li> <li> <p>Return to the Azure AI Foundry portal and click on Overview in sidebar</p> </li> <li> <p>Copy over the values from that page, to update the <code>.env</code> file</p> <ul> <li><code>AZURE_OPENAI_KEY</code> - set the value from the <code>API Key</code> panel</li> <li><code>AZURE_OPENAI_ENDPOINT</code> - set the value from the <code>Azure OpenAI Service</code> capability</li> </ul> <p>The endpoint should be in the form \"https://XXXX.openai.azure.com/\"</p> </li> </ol> <p>You are now ready to start exploring reasoning models from both playground and code</p>"},{"location":"01-Workshop/01-Introduction/01/","title":"Reasoning Models","text":"<p>BY THE END OF THIS SECTION YOU SHOULD KNOW</p> <ol> <li>What reasoning models are and the scenarios they are best suited for</li> <li>The reasoning models available in Azure AI Foundry model catalog</li> <li>The reasoning models available in GitHub Models Marketplace</li> </ol> <p>IN-VENUE ATTENDEES: This content will be covered by the instructor talk. Skip directly to Lab 3</p> <p>Reasoning models are a new category of Large Language Models that are trained (with large-scale reinforcement learning) to think deeply before they respond. These reasoning abilities are achieved by a combination of techniques including chain-of-thought, self-consistency, and deliberative alignment. </p> <p>Chain-of-thought prompting encourages the model to break down complex problems into smaller, manageable steps, while self-consistency allows the model to evaluate multiple solutions and select the most accurate one. Deliberative alignment helps models to reason about their own behaviors, helping to improve the safety and reliability of their responses.</p> <p>In this workshop, we'll primarily use the Azure OpenAI  o-series of models for hands-on labs. However, the concepts translate broadly to reasoning models from other providers including DeepSeek and Microsoft</p>"},{"location":"01-Workshop/01-Introduction/01/#what-can-they-do","title":"What can they do?","text":"<p>Reasoning models are designed to tackle hard problems involving logic, stratgegy, complex reasoning and multi-step planning. They are particularly effective in STEM fields (math, science, coding) where they can outperform popular general-purpose models in competitive benchmarks.</p> <p>As shown in these charts, o3 and o4-mini (the latest reasoning models from OpenAI) are capable of achieving state-of-the-art performance on a variety of reasoning tasks. </p> <ol> <li>The PhD Model - reached 83.3% accuracy on PhD-level science questions (GPQA Diamond)</li> <li>The Mathlete - reached 92.7% accruracy on competitive Olympiad-level Math (AIME 2025)</li> <li>The Coder - reached 69% accuracy in SWE benchmark, earned $65K in freelance benchmark</li> <li>The Multimodal Reasoner - reached 86.8% accuracy on visual math reasoning problems</li> </ol> <p>The ability to bring multimodal reasoning to AI applications can be a game-changer for many industries - expanding the scope to new domains like healthcare, finance, and education. </p>"},{"location":"01-Workshop/01-Introduction/01/#why-is-this-important","title":"Why is this important?","text":"<p>The stats above are impressive, but they don't tell the whole story. Reasoning models are not just about achieving high accuracy on benchmarks. They also offer significant advantages in terms of cost and speed. For instance, data shows that the o3 and o4-mini models are significantly faster and cheaper than the <code>o1</code> series that came before. If the trend continues, they will become more competitive with general-purpose models and gain more adoption in production apps. </p> <p></p> <p>So, how can you get hands-on experience with these models? Let's talk about two options.</p>"},{"location":"01-Workshop/01-Introduction/01/#azure-ai-foundry-models","title":"Azure AI Foundry models","text":"<p>The Azure AI Foundry model catalog has a rich set of options that you can explore for use in production. Here are a few of the popular options:</p> <ol> <li>MAI-DS-R1 - a DeepSeek-R1 reasoning model post-trained by the Microsoft AI team </li> <li>o4-mini- the most efficient reasoning model in the o-series, good for agentic use</li> <li>o3 - the most capable reasoning model in the o-series, first to support tool use</li> <li>o3-mini - the faster and more cost-efficient version of o3</li> <li>DeepSeek R1 - combined reinforcement learning with fine-tuning on curated datasets</li> <li>o1-mini - the faster and more cost-efficient version of o1</li> <li>o1 - the first  reasoning model from OpenAI, launching the o-series</li> </ol> <p>For our hands-on labs, we'll prioritize o4-mini but compare with o1 or gpt-4o-mini as needed</p>"},{"location":"01-Workshop/01-Introduction/01/#github-marketplace-models","title":"GitHub Marketplace models","text":"<p>But what if you just want to get a sense for the models and don't yet have an Azure subscription? The GitHub Models Marketplace collection may be your best starting point. These are Azure AI Foundry models that you can access for free with just a GitHub account! They support a serverless endpoint that can be accessed from the browser (playground) or from code (API), making it easy to go from ideation to implementation while you learn. </p> <p>Keep in mind:</p> <ol> <li>GitHub Models are rate-limited. Use the models for learning, not for production.</li> <li>Migrating to Azure later is easy. Just use the Azure AI model inference service.</li> </ol>"},{"location":"01-Workshop/01-Introduction/02/","title":"Lab 3: Compare Models","text":"<p>BY THE END OF THIS SECTION YOU SHOULD HAVE</p> <ol> <li>Some intuition for how reasoning differs from general-purpose models</li> <li>Hands-on experience with the GitHub Models playground</li> <li>Hands-on experience with prompting a reasoning model</li> </ol> <p>Let's see you have an enterprise application scenario in mind. What model should you use? Let's do a practical exercise next and use our observations to build some intuition.</p>"},{"location":"01-Workshop/01-Introduction/02/#explore-github-models","title":"Explore GitHub Models","text":"<p>For this exercise, we'll make use of the side-by-side comparison feature of GitHub models.</p> <ol> <li>Open your browser to a new tab and visit this link. Log into GitHub if prompted.</li> <li>You see available reasoning models, sorted by popularity. Select MAI-DS-R1</li> <li>You see the model card with a Playground option. Click to open model in playground</li> <li>You see a \"Compare models\" option. Select GPT-4o-mini from the dropdown.</li> </ol> <p>You'll see both models run side-by-side</p> <ol> <li>Type a prompt in the input dialog on one side</li> <li>Both models will execute that prompt in sync</li> <li>Watch closely to see how they compare in response latency, quality, and token costs</li> </ol> <p>Ready? Try the 2 prompts below (one per tab) and see if your observations match mine.</p>"},{"location":"01-Workshop/01-Introduction/02/#1-simple-question","title":"1. Simple Question","text":"<p>PROMPT 1: Which movie has the most Oscars?</p> <p>This is a simple question-answering task, something that GPT models are exceptionally good at. Let's make some observations. Since models are stochastic, your response may look different.</p> <ol> <li>GPT model was faster (1.6s vs. 9.8s)</li> <li>It used less output tokens (92 vs. 767)</li> <li>Both got the answers right - but reasoning model was overthinking it</li> </ol> <p>Now switch to the other tab.</p> <p></p>"},{"location":"01-Workshop/01-Introduction/02/#2-simple-reasoning","title":"2. Simple Reasoning","text":"<p>PROMPT 2: How many ps are in the word hippopotamus?</p> <p>This is a little trickier since it requires the model to think about the question before it can respond. This is where reasoning models have an edge. This is a trivial example (for illustration purposes) but we can still make relevant observations:</p> <ol> <li>GPT model was faster (0.6s vs. 6.1s) - but it gets the answer wrong!</li> <li>Reasoning model used way more tokens (447 vs. 15) - to respond.</li> <li>Reasoning model \"shows\" its reasoning within <code>&lt;think&gt;...&lt;think&gt;</code> tags</li> <li>It reasons step-by-step (without being prompted to do so)</li> <li>It tries to verify its thinking in different ways before responding</li> </ol> <p></p>"},{"location":"01-Workshop/01-Introduction/02/#how-does-reasoning-differ","title":"How does reasoning differ?","text":"<p>Reasoning models have unique characteristics in how they respond:</p> <ol> <li> <p>Explicit Step-by-Step Processing: They break down complex problems into manageable steps, working through each component logically before arriving at a conclusion.</p> </li> <li> <p>Built-in Chain-of-Thought: They demonstrate their reasoning chains explicitly, allowing users to follow their problem-solving approach and verify the logic.</p> </li> <li> <p>Built-in Self-Correction: Advanced reasoning models can identify errors in their own thinking, backtrack, and correct their reasoning path when necessary.</p> </li> </ol> <p>But they also introduce new tradeoffs we need to take into account.</p> <ol> <li>Token Costs: Reasoning models incur added higher token costs for sharing their reasoning.</li> <li>Latency: Reasoning models take time to think - incurring added delays in response.</li> <li>Precision &amp; Accuracy: Reasoning models are more reliable - they verify &amp; self-correct.</li> </ol>"},{"location":"01-Workshop/01-Introduction/02/#when-do-we-use-it","title":"When do we use it?","text":"<p>How should we choose between these two classes of models, when starting a new project? There are two ways to think about this.</p> <ol> <li>By task type - GPT models are good at straightforward tasks, while reasoning models excel at specialized problem-solving. GPT models are \"doers\". Reasoning models are \"planners\".</li> <li>By core priority - GPT models are typically faster and cheaper, while reasoning models are more accurate and reliable. If accuracy is the top priority, reasoning models get the edge. </li> </ol> <p>In reality, general-purpose models are increasingly able to work on simpler reasoning tasks, and reasoning models are getting cheaper and faster with every new generation. To make a more informed decision, it helps to know how reasoning models work under the hood.</p>"},{"location":"01-Workshop/01-Introduction/03/","title":"Lab 4: Manage Tokens","text":"<p>BY THE END OF THIS SECTION YOU SHOULD KNOW</p> <ol> <li>What reasoning tokens are, and how they relate to inference time scaling</li> <li>Why token management is critical, and the levers available to influence it </li> <li>How to use a trial and error approach to find the right token management strategy</li> </ol> <p>Reasoning models are designed to think deep and hard about complex problems - where precision and accuracy of responses are a higher priority. The tradeoff we make is in response latency and token usage. In this section, we'll see where these reasoning overheads occur, and what knobs and dials we have to optimize them.</p>"},{"location":"01-Workshop/01-Introduction/03/#reasoning-tokens","title":"Reasoning Tokens","text":"<p>Reasoning models work by introducing the idea of reasoning tokens as shown below. </p> <ol> <li>These tokens represent the \"thinking output\" of the model at a given step (turn) </li> <li>Tokens are not carried over, but they count against the model context window.</li> <li>Increasing the count of reasoning tokens encourages model to \"think more\"</li> <li> <p>Increasing it too much can cause it to starve output tokens (reducing quality)</p> <p></p> </li> </ol>"},{"location":"01-Workshop/01-Introduction/03/#token-management","title":"Token Management","text":"<p>So how do we we strike the right balance between reasoning accuracy (more reasoning tokens) and output quality (fewer reasoning tokens)? We have a few knobs we can turn:</p> <ol> <li>Set max_completion_tokens to limit the total number of tokens generated per turn - including both visible (output) and hidden (reasoning) tokens.</li> <li>Set reasoning_effort to <code>low</code>, <code>medium</code>, or <code>high</code> based on your requirements. The default is medium - allowing you to move the dial up or down, to optimize for accuracy or cost.</li> <li>Try different reasoning models (e.g., old-vs-new, full-vs-mini). The token cost or response quality may vary even if all other parameters are the same.</li> </ol> <p>This may require trial-and-error strategies. We'll see an example in practice below.</p>"},{"location":"01-Workshop/01-Introduction/03/#inference-time-scaling","title":"Inference Time Scaling","text":"<p>Let's briefly talk about inference time scaling and how it relates to reasoning effort.</p> <ol> <li>Training time compute refers to the time and resources needed to train the LLM to do reasoning. Training compute is expensive. So, even though increasing training time improves performance, we cannot afford to do so at will (e.g., for specific tasks)</li> <li>Inference time compute refers to the time and resources needed to execute a request with that trained model. Inference compute is cheaper than training, and data indicates that giving models more time to think (inference time) may lead to better performance. This makes inference time scaling the more cost-effective way to adapt and grow performance.</li> </ol> <p>Reasoning models can use various strategies to scale inference time, including:</p> <ul> <li>Increasing count of reasoning tokens (give it more space to think)</li> <li>Asking the model to wait (and assess status before it continues)</li> <li>Asking the model to explore strategies in parallel (encourage more thought)</li> </ul>"},{"location":"01-Workshop/01-Introduction/03/#task-visual-reasoning","title":"Task: Visual Reasoning","text":"<p>Let's revisit the visual reasoning example in the o4-mini release announcement. The use case involves this image of a person holding up a picture with a tour schedule - and uses this prompt: </p> <p>It's 12, I've already seen #4,  output a planning that will make sure I see all attractions and shows,  taking into account their duration (1st column)  and a 10 minute buffer between each show.</p> <p></p> <p>To explore the levers available to us for token management, let's start with a Playground exercise where we adjust three levers: reasoning effort, max completion tokens, and choice of reasoning model.</p> <p>We have two reasoning model deployments in our Azure AI Foundry project: <code>o1</code> and <code>o4-mini</code>. </p> <p>Both models can handle multimodal inputs, so let's try to figure out which one is best for a visual reasoning task based on the tradeoffs between token cost and accuracy. Note that the model is not just describing the picture, it is actively reasoning about how to use that information to fulfill the user's objective.</p>"},{"location":"01-Workshop/01-Introduction/03/#lab-instructions","title":"Lab Instructions","text":"<ol> <li>You'll find a copy of this image here: <code>docs/Workshop/assets/example-o1.webp</code>. Save the image to your laptop by right-clicking on the image shown above and using the \"Save As\" action.</li> <li>Use the Azure AI Foundry Playground to test the prompt with each of the models <code>o1</code> and <code>o4-mini</code> in turn</li> <li>In each step, copy the prompt above into the prompt box (top left)</li> <li>Click the paperclip icon in the prompt box to upload the image above</li> <li>Open the \"Parameters\" control to set the reasoning effort to the desired level (low, medium, high)</li> <li>Set the max completion tokens to the desired length (100K, 40K, 4K)</li> <li>Run the prompt and observe the response and token usage </li> </ol> <p>Detailed steps are provided for each model / reasoning effort combination below.</p> <p>Based on your observations -  what do you feel is the best model for this task?</p>"},{"location":"01-Workshop/01-Introduction/03/#1-o4-mini-low100k","title":"1. o4-mini (low)(100K)","text":"<ol> <li>Use the faster, cheaper o4-mini model with the default max completion tokens (100K)</li> <li>Set reasoning effort to low</li> <li>Response took 632 tokens</li> </ol>"},{"location":"01-Workshop/01-Introduction/03/#2-o4-mini-high100k","title":"2. o4-mini (high)(100k)","text":"<ol> <li>Set reasoning effort to high</li> <li>Response took 642 tokens</li> <li>We observe that changing reasoning effort did not improve performance much. This may be because the model was able to get the best answer with the default reasoning effort.</li> </ol>"},{"location":"01-Workshop/01-Introduction/03/#3-o1-high40k","title":"3. o1 (high)(40K)","text":"<ol> <li>Change models to o1 (old-vs-new)(full-vs-mini)</li> <li>Note that o1 suggests lower completion tokens (40K) by default</li> <li>Note that o1 also lacks the Generate Summary parameter</li> <li>We observe that the model uses more tokens but takes additional effort to provide tips.</li> </ol>"},{"location":"01-Workshop/01-Introduction/03/#4-o1-high4k","title":"4.  o1 (high)(4K)","text":"<ol> <li>Let's lower the max completion tokens to 4K</li> <li>Note that this impacts model's ability to think - no response is generated</li> </ol>"},{"location":"01-Workshop/01-Introduction/03/#5-o1-low4k","title":"5. o1 (low)(4K)","text":"<ol> <li>Keep the max completion tokens at 4K</li> <li>Set reasoning effort to low</li> <li>We do get a response - but it's the most tokens used so far.</li> </ol> <p>The main takeaway is to experiment with different models and parameters to find the right balance for your use case. In this case, we see that o4-mini with default (high reasoning) has the best value.</p>"},{"location":"01-Workshop/02-Ideation/01/","title":"Lab 5 \u00b7 Prompt Engineering","text":"<p>BY THE END OF THIS SECTION YOU SHOULD KNOW</p> <ol> <li>How prompt engineering for reasoning models differs from the normal guidance</li> <li>How to track tradeoffs in both response quality and model performance</li> <li>Why token management is critical, and the levers available to influence it </li> <li>How to use a trial and error approach to find the right token management strategy</li> </ol> <p>REMINDER: These models are stochastic. Your response may not match the sample exactly</p>"},{"location":"01-Workshop/02-Ideation/01/#prompting-principles","title":"Prompting Principles","text":"<p>First, let's review recommended prompting practices for these models.</p> <ol> <li>Use developer messages to set context.</li> <li>Keep prompts clear and direct.</li> <li>Avoid chain-of-thought; models reason internally.</li> <li>Use delimiters to separate sections.</li> <li>Try zero-shot first; add examples only if needed.</li> <li>Clearly state any output constraints.</li> <li>Be precise about goals and success criteria.</li> <li>Include \u201cFormatting re-enabled\u201d to request markdown output.</li> </ol> <p>Over the rest of this workshop, we'll follow a basic practice.</p> Text Only<pre><code>- Describe a use case or scenario\n- Start with a simple (and potentially messy) prompt. \n- Think about applying at least one prompting principle to refine it.\n- Assess the impact in terms of quality, token costs and latency.\n</code></pre> <p>This will help you gradually build up intuition for when to apply each principle effectively.</p>"},{"location":"01-Workshop/02-Ideation/01/#use-case-math-tutor","title":"Use Case: Math Tutor","text":"<p>First, let's define a scenario. We know the reasoning models are good at math and logic, and the o-series models are notably good at competitive math challenges like AIME. Let's use that.</p> <p>Scenario: I am a Math tutor trying to help USA Computing Olympiad students</p> <ol> <li>Here's an example of a USA Computing Olympiad math problem that requires reasoning. There are hundreds of these problems available to prospective students - allowing them to sharpen their skills.</li> <li>I have this in a PDF format. I want it to extract the problem statement and solve it, and then write up an explanation for me of that solution.</li> <li>Then I want it to think of a better way to explain this visually by writing code that could generate a diagram or image that helps whiteboard this for visual learners.</li> </ol>"},{"location":"01-Workshop/02-Ideation/01/#1-set-clear-context","title":"1. Set Clear Context","text":"<p>You are a math tutor.</p> <p>You are helping a student prepare for the USA Computing Olympiad. You will extract the problem statement from the PDF and solve it. Then you will write up an explanation of the solution. Finally, you will think of a better way to explain this visually by writing code that could generate a diagram or image that helps whiteboard this for visual learners.</p> <ol> <li>Describe your role or persona</li> <li>Describe the model's primary goal</li> <li>Provide it any clarifying instructions</li> </ol>"},{"location":"01-Workshop/02-Ideation/01/#2-set-model-a-task","title":"2. Set Model a Task","text":"<ul> <li>Upload the PDF to the Playground</li> <li>Enter the prompt: <code>Here's the PDF</code></li> </ul> <p>RESPONSE: Model responds with a message saying it cannot read the PDF (via Playground)</p>"},{"location":"01-Workshop/02-Ideation/01/#21-iterate-copy-data","title":"2.1 Iterate: Copy Data","text":"<ul> <li>Copy the problem directly into the prompt: </li> <li>Note: this uses model context window</li> <li>What do you observe?</li> </ul> Click to expand for the full prompt (to copy) <pre><code>Bessie the cow used to have such a nice view from her barn, looking across road at a set of two billboards advertising delicious looking cow feed. Unfortunately, one of these billboards has recently been updated so it now advertises \"Farmer Larry's Lawnmowers\". Bessie is not a fan of lawnmowers since their only purpose, as far as she can tell, is cutting back the grass in her field that she finds so tasty (if you haven't noticed, much of Bessie's thought process revolves around food).\n\nFortunately, the remaining cow feed billboard is situated in front of the lawnmower billboard, potentially obscuring it.\n\nBessie, determined to remove the offensive lawnmower billboard completely from her view, hatches a risky plan. She plans to steal a large rectangular tarp from the barn and sneak out late at night to cover the remaining portion of the lawnmower billboard, so that she can no longer see any part of it.\n\nGiven the locations of the two billboards, please help Bessie compute the minimum area of the tarp she will need. Since the only tarps available in the barn are rectangular in size, Bessie observes that she may conceivably need a tarp whose area is slightly larger than the exposed area of the lawnmower billboard, as illustrated in the example below. The tarp may only be placed such that its sides are parallel to those of the other billboards (i.e., it cannot be \"tilted\").\n\nINPUT FORMAT (file billboard.in):\nThe first line of input contains four space-separated integers: x1\ny1\nx2\ny2\n, where (x1,y1)\nand (x2,y2)\nare the coordinates of the lower-left and upper-right corners of the lawnmower billboard in Bessie's 2D field of view. The next line contains four more integers, similarly specifying the lower-left and upper-right corners of the cow feed billboard. The cow feed billboard may obscure all, some, or none of the lawnmower billboard. All coordinates are in the range -1000 to +1000.\nOUTPUT FORMAT (file billboard.out):\nPlease output the minimum area of the tarp Bessie needs to use to cover part of the lawnmower billboard so that it becomes completely obscured.\nSAMPLE INPUT:\n2 1 7 4\n5 -1 10 3\nSAMPLE OUTPUT:\n15\nHere, the cow feed billboard obscures the lower right corner of the lawnmower billboard, but this doesn't really help, since Bessie still needs to use a tarp whose size is as large as the entire lawnmower billboard.\n\nProblem credits: Brian Dean\n</code></pre>"},{"location":"01-Workshop/02-Ideation/01/#22-accuracy-vs-cost","title":"2.2 Accuracy vs Cost","text":"<ol> <li>You may see a response like this.</li> <li>Note the Cost in tokens e.g., <code>2477</code> tokens</li> <li>Did the response meet your request? e.g., is it visual?</li> <li>Can you improve this? e.g., get it to draw the picture?</li> </ol> Click to expand for a sample response <p>Below is the full problem statement, a worked\u2010out solution, and finally a small Python script (using matplotlib) you can use to generate a diagram illustrating any test case.</p> <p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   Problem (\u201cBlocked Billboard\u201d)   \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   Bessie the cow used to have such a nice view from her barn, looking across the road at two billboards advertising delicious cow feed. One of these billboards has recently been updated and now advertises \u201cFarmer Larry\u2019s Lawnmowers.\u201d Bessie dislikes lawnmowers and would like to completely obscure the offending lawnmower billboard from her view by placing one large rectangular tarp over whatever portion of it is still visible (the other billboard may already cover part of it).</p> <p>You are given the coordinates of the lower\u2010left and upper\u2010right corners of:</p> <p>the lawnmower billboard   the cow\u2010feed billboard   All rectangles are axis\u2010aligned. Compute the minimum area of a single axis\u2010aligned rectangular tarp that will cover all of the still\u2010visible portion of the lawnmower billboard. If the feed billboard already completely covers the lawnmower one, answer 0. If there is no overlap, you must cover the entire lawnmower billboard.</p> <p>Input (billboard.in)   Line 1: x1 y1 x2 y2 \u2014 lawnmower billboard corners   Line 2: x3 y3 x4 y4 \u2014 feed billboard corners   (\u20131000 \u2264 all coords \u2264 +1000)</p> <p>Output (billboard.out)   A single integer: the minimum tarp area needed.</p> <p>Sample Input   2 1 7 4   5 \u20131 10 3</p> <p>Sample Output   15</p> <p>Explanation of the sample:</p> <p>Lawn mower billboard spans (2,1) to (7,4) \u2192 area = 5\u00d73 = 15.   Feed billboard spans (5,\u20131) to (10,3), which overlaps the lower right corner of the mower but does not cover it fully in either dimension.   The remaining visible portions of the mower are in two disconnected pieces, so any single rectangle that contains both must be as large as the entire mower (area 15).</p> <p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   Solution Outline   \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   Let   mower = [X1,Y1]\u00d7[X2,Y2],   feed = [A1,B1]\u00d7[A2,B2].   Compute mower width and height:   W = X2 \u2013 X1   H = Y2 \u2013 Y1   Full mower area = W \u00d7 H.</p> <p>Compute overlap dimensions:   overlap_width = max(0, min(X2,A2) \u2013 max(X1,A1))   overlap_height = max(0, min(Y2,B2) \u2013 max(Y1,B1))   If overlap_width == 0 or overlap_height == 0, there is no intersection \u2192 tarp must cover whole mower \u2192 answer = W\u00d7H.</p> <p>If the feed fully covers the mower:   A1 \u2264 X1, A2 \u2265 X2, B1 \u2264 Y1, and B2 \u2265 Y2   \u2192 answer = 0.</p> <p>Otherwise there is a partial overlap. The only time you can cover the remaining visible piece with less area than the full mower is when the feed \u201cclips\u201d off exactly one side (top, bottom, left, or right), leaving one contiguous rectangle. Concretely:   \u2022 If feed\u2019s x\u2010interval covers mower\u2019s full width (A1 \u2264 X1 and A2 \u2265 X2), then feed is a horizontal strip.   \u2013 If it touches the bottom of the mower (B1 \u2264 Y1 &lt; B2 &lt; Y2), it clips off the bottom \u2192 one piece remains at top of height Y2\u2013B2, so tarp area = W \u00d7 (Y2\u2013B2).   \u2013 If it touches the top of the mower (Y1 &lt; B1 &lt; Y2 \u2264 B2), it clips off the top \u2192 one piece remains at bottom of height B1\u2013Y1, so tarp area = W \u00d7 (B1\u2013Y1).</p> <p>\u2022 Else if feed\u2019s y\u2010interval covers mower\u2019s full height (B1 \u2264 Y1 and B2 \u2265 Y2), then feed is a vertical strip.   \u2013 If it touches the left side (A1 \u2264 X1 &lt; A2 &lt; X2), tarp area = H \u00d7 (X2\u2013A2).   \u2013 If it touches the right side (X1 &lt; A1 &lt; X2 \u2264 A2), tarp area = H \u00d7 (A1\u2013X1).</p> <p>Otherwise the feed sits \u201cin the middle\u201d of the mower leaving two or more disconnected pieces \u2192 the minimal enclosing rectangle is the entire mower \u2192 area = W\u00d7H.</p> <p>Putting it all together, compute these cases and take the minimum area you find.</p> <p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   Sample Implementation (C++\u2010style pseudocode)   \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014</p> <p>read X1,Y1,X2,Y2   read A1,B1,A2,B2  </p> <p>W = X2 - X1   H = Y2 - Y1   fullArea = W * H  </p> <p>// compute overlap   ovW = max(0, min(X2,A2) - max(X1,A1))   ovH = max(0, min(Y2,B2) - max(Y1,B1))  </p> <p>// no intersection   if (ovW == 0 or ovH == 0)    print fullArea and exit  </p> <p>// feed covers mower completely   if (A1 &lt;= X1 and A2 &gt;= X2 and B1 &lt;= Y1 and B2 &gt;= Y2)    print 0 and exit  </p> <p>ans = fullArea  </p> <p>// horizontal strip case   if (A1 &lt;= X1 and A2 &gt;= X2) {   // bottom strip   if (B1 &lt;= Y1 and B2 &lt; Y2)       ans = min(ans, W * (Y2 - B2))   // top strip   if (B2 &gt;= Y2 and B1 &gt; Y1)       ans = min(ans, W * (B1 - Y1))   }  </p> <p>// vertical strip case   if (B1 &lt;= Y1 and B2 &gt;= Y2) {   // left strip   if (A1 &lt;= X1 and A2 &lt; X2)        ans = min(ans, H * (X2 - A2))   // right strip   if (A2 &gt;= X2 and A1 &gt; X1)        ans = min(ans, H * (A1 - X1))   }  </p> <p>print ans  </p> <p>Time complexity is O(1) and uses only a few comparisons and arithmetic operations.</p> <p>\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   Visual Aid Generator (Python + matplotlib)   \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014   Here is a little script you can run to draw any test case. It plots the mower in red outline, the feed in semi\u2010transparent blue, and shades in green the smallest tarp you\u2019d need.</p> <p>import matplotlib.pyplot as plt   import matplotlib.patches as patches  </p> <p>def draw_case(mower, feed):      \"\"\"      mower and feed are tuples (x1,y1,x2,y2).      Draws the mower (red outline), feed (blue fill), and      minimal tarp (green fill).      \"\"\"      x1,y1,x2,y2 = mower      a1,b1,a2,b2 = feed  </p> Text Only<pre><code> fig, ax = plt.subplots()  \n ax.set_aspect('equal')\n\n # Draw mower billboard (red outline)  \n mower_rec = patches.Rectangle((x1, y1), x2-x1, y2-y1,  \n                               edgecolor='red', facecolor='none', lw=2)  \n ax.add_patch(mower_rec)\n\n # Draw feed billboard (blue translucent)  \n feed_rec = patches.Rectangle((a1, b1), a2-a1, b2-b1,  \n                               edgecolor='blue', facecolor='blue', alpha=0.3)  \n ax.add_patch(feed_rec)\n\n # Compute tarp rectangle  \n # First apply the same logic as solution to find its coords  \n ovw = max(0, min(x2,a2) - max(x1,a1))  \n ovh = max(0, min(y2,b2) - max(y1,b1))  \n # By default tarp covers entire mower  \n tx1, ty1, tx2, ty2 = x1, y1, x2, y2\n\n # no overlap -&gt; tarp = mower  \n if ovw &gt; 0 and ovh &gt; 0:  \n    # fully covered?  \n    if not (a1 &lt;= x1 and a2 &gt;= x2 and b1 &lt;= y1 and b2 &gt;= y2):  \n          # check horizontal strip  \n          if a1 &lt;= x1 and a2 &gt;= x2:  \n             if b1 &lt;= y1 and b2 &lt; y2:  \n                ty1 = b2  \n             elif b2 &gt;= y2 and b1 &gt; y1:  \n                ty2 = b1  \n          # check vertical strip  \n          if b1 &lt;= y1 and b2 &gt;= y2:  \n             if a1 &lt;= x1 and a2 &lt; x2:  \n                tx1 = a2  \n             elif a2 &gt;= x2 and a1 &gt; x1:  \n                tx2 = a1\n\n # Draw tarp (green translucent)  \n tarp_rec = patches.Rectangle((tx1, ty1), tx2-tx1, ty2-ty1,  \n                               edgecolor='green', facecolor='green', alpha=0.3)  \n ax.add_patch(tarp_rec)\n\n # set plot limits a bit beyond the union of rectangles  \n margin = 1  \n min_x = min(x1, a1) - margin  \n max_x = max(x2, a2) + margin  \n min_y = min(y1, b1) - margin  \n max_y = max(y2, b2) + margin  \n ax.set_xlim(min_x, max_x)  \n ax.set_ylim(min_y, max_y)\n\n plt.xlabel('X')  \n plt.ylabel('Y')  \n plt.title('Red = LawnMower, Blue = Feed, Green = Tarp')  \n plt.grid(True)  \n plt.show()\n</code></pre> <p># Example usage:   if name == 'main':      # Sample from problem statement      mower = (2,1,7,4)      feed  = (5,-1,10,3)      draw_case(mower, feed)  </p> <p>How to use the script:   \u2022 Change the mower and feed tuples to any coordinates you like.   \u2022 Run python visualize.py (assuming you save it that name).   \u2022 You\u2019ll see a plot showing how the feed covers the mower and exactly what tarp you need.</p> <p>This visual should make it crystal\u2010clear in each case why the tarp can sometimes be smaller (a single clipped side), or otherwise must cover the full lawnmower billboard.</p>"},{"location":"01-Workshop/02-Ideation/01/#3-homework-try-this","title":"3. Homework: Try this","text":"<p>EXPLORE AT HOME: Iterate and build intuition for yourself.</p>"},{"location":"01-Workshop/02-Ideation/01/#31-few-shot-thinking","title":"3.1 Few Shot Thinking","text":"<ul> <li>Above prompts were zero-shot in guiding code choices</li> <li>Try rewriting the prompt by suggest Python plotly to visualize it</li> <li>How does this impact response quality and cost?</li> </ul>"},{"location":"01-Workshop/02-Ideation/01/#32-use-delimiters","title":"3.2 Use Delimiters","text":"<ul> <li>Use xml tags to separate word problem, data and suggestions</li> <li>Use tag names that clearly reflect context or provide added insight</li> <li>How does this impact response quality and cost?</li> </ul>"},{"location":"01-Workshop/02-Ideation/01/#33-state-constraints","title":"3.3 State Constraints","text":"<ul> <li>The response is verbose - ask it to use bullet points</li> <li>Specify languages for USACO - C++, Java and Python</li> <li>Specify desired output format - display plot on console</li> <li>How does this impact response quality and cost?</li> </ul>"},{"location":"01-Workshop/02-Ideation/02/","title":"Lab 6 \u00b7 Visual Reasoning","text":"<p>BY THE END OF THIS SECTION YOU SHOULD HAVE</p> <ol> <li>Some intuition for the difference between visual and text reasoning problems</li> <li>Some experience using multimodal prompts for visual reasoning in Playground</li> <li>Some intuition for good visual reasoning problems and prompt engineering them</li> </ol> <p>Visual Reasoning is when the model is not just analyzing the picture (object recognition), it is actively using it for reasoning. This is a fascinating new area for reasoning applications - so let's use this lab to explore a small example.</p> <p>REMINDER: These models are stochastic. Your response may not match the sample exactly</p>"},{"location":"01-Workshop/02-Ideation/02/#1-prompt","title":"1. Prompt","text":"<p>Here is a good example of a visual reasoning challenge - a word jumble. The image come from  this Wikipedia article.</p> <ol> <li>Upload the image to the Playground.</li> <li>Use the prompt:</li> </ol> <pre><code>\"Solve this\"\n</code></pre> <p></p>"},{"location":"01-Workshop/02-Ideation/02/#2-first-response","title":"2. First Response","text":"<p>HOW DID YOUR MODEL RESPOND?</p> <p>Here's what we see in the sample response below:</p> <ol> <li>It got the answer right: No Chemistry </li> <li>Note that the model was configured for detailed response and showed its work</li> <li>The response took 544 tokens with high reasoning effort.</li> <li>It also recognized that the puzzle was humorous (cites the \"punchline\")</li> </ol> <p></p>"},{"location":"01-Workshop/02-Ideation/02/#3-dont-generate-summary","title":"3. Don't Generate Summary","text":"<p>ITERATE ON THE PROMPT - NOTE YOUR OBSERVATIONS</p> <ol> <li>Repeat (2) but set generate summary option to \"none\"?</li> <li>Observe what happens now. For example I noticed:<ol> <li>The response took 345 tokens with high reasoning effort.</li> <li>The output was shorter and less detailed than the previous one</li> </ol> </li> </ol> <pre><code>First you unscramble each word:\n\nYNAROC \u2192 CRAYON\nDISNM \u2192 MINDS\nNYNIT \u2192 TINNY\nARHCEB \u2192 BREACH\n\nThen you take the letters in the circled positions of each (3 from CRAYON, 3 from MINDS, 2 from TINNY, 3 from BREACH) to get\n\nC Y O  M I S  T N  R E H\n\nFinally you anagram those eleven letters into the punny answer:\n\nNO CHEMISTRY\n</code></pre>"},{"location":"01-Workshop/02-Ideation/02/#4-low-reasoning-effort","title":"4. Low Reasoning Effort","text":"<p>ITERATE ON THE PROMPT - NOTE YOUR OBSERVATIONS</p> <ol> <li>Repeat (2) - and also set reasoning effort option to \"low\"?</li> <li>Some observations:<ul> <li>The answer was still correct - but took more (389) tokens</li> </ul> </li> </ol>"},{"location":"01-Workshop/02-Ideation/02/#5-continued-conversation","title":"5. Continued Conversation","text":"<p>ITERATE ON THE PROMPT - NOTE YOUR OBSERVATIONS</p> <ol> <li>Let's see if the model understand the nuances of this puzzle.</li> <li>Set the reasoning effort to: high</li> <li>Set the generate summary to: yes</li> <li> <p>Try this prompt:</p> <pre><code>\"create a new puzzle like that one - and explain how you did it\"\n</code></pre> </li> <li> <p>Some observations:</p> <ol> <li>The response took  953 tokens with high reasoning effort.</li> <li>It not only created the puzzle and explained the reasoning, it understood the pattern and made sure there was a punny answer.</li> </ol> </li> </ol> SEE SAMPLE RESPONSE: Click to expand <p>Here\u2019s a \u201cbreak\u2011up pun\u201d puzzle along the same lines:</p> <p>PUZZLE   Unscramble each row into an ordinary word. In the finished word, the letters in circled boxes are to be collected (in any order) to spell the two\u2011word punch\u2011line.</p> <p>O M L A R   \u25a1\u25ef\u25a1\u25ef\u25ef   (unscrambles to ______ )</p> <p>P A R E S   \u25ef\u25a1\u25ef\u25a1\u25a1   (unscrambles to ______ )</p> <p>A T P I N   \u25ef\u25ef\u25a1\u25a1\u25a1   (unscrambles to ______ )</p> <p>C E O N   \u25a1\u25a1\u25ef\u25ef   (unscrambles to ______ )</p> <p>CARTOON CAPTION   \u201cThe geologist broke up with the astronomer because they needed __.\u201d</p> <p>Print the 9 circled letters here:   \u25a1\u25a1 \u25a1\u25a1\u25a1\u25a1\u25a1</p> <p>SOLUTION SKETCH</p> <p>OMLAR \u2192 MORAL   circled letters (positions 1, 2, 3) \u2192 M O R   PARES \u2192 SPEAR   circled letters (positions 1, 3) \u2192 S E   ATPIN \u2192 PAINT   circled letters (positions 1, 2) \u2192 P A   CEON \u2192 ONCE   circled letters (positions 3, 4) \u2192 C E   Collected letters are M O R E S P A C E \u2192 \u201cMORE SPACE.\u201d</p> <p>So the finished caption reads:   \u201cThe geologist broke up with the astronomer because they needed MORE SPACE.\u201d</p> <p>HOW I MADE IT</p> <p>Pick a short punny phrase (\u201cMORE SPACE\u201d) for the cartoon punch\u2011line.   List its letters (M,O,R,E,S,P,A,C,E) and decide how many circles to draw on each of four words (here 3+2+2+2 = 9).   Choose four everyday words that contain exactly the needed circled letters.   Scramble each word and mark which positions will be circled.   Lay out the puzzle with blanks, circles, and the cartoon clue so that solvers unscramble, extract the circled letters, then re\u2011scramble them into the final joke answer.</p>"},{"location":"01-Workshop/02-Ideation/02/#6-switch-task-focus","title":"6. Switch Task Focus","text":"<p>STAY IN CONVERSATION BUT SWITCH TASK - NOTE YOUR OBSERVATIONS</p> <p>Let's keep the conversation history but use a different image type (webp) and task (calorie estimate). We'll use this image of a lava cake from the public domain.</p> <ol> <li>Upload this <code>webp</code> image to the Playground (in the prompt console)</li> <li> <p>Try this prompt:</p> <pre><code>Can you estimate the calories in this?\n</code></pre> </li> <li> <p>Did the model make the context switch to the new task? Some observations:</p> <ol> <li>The model was able to recognize the image and estimate the calories in it.</li> <li>It gave a detailed breakdown of ingredients and respective calorie counts.</li> <li>Token Cost: 572</li> </ol> </li> </ol> <p></p> <p>Response</p> <pre><code>  Here\u2019s a rough breakdown based on typical portion sizes:\n\n     \u2022 Chocolate \u201clava\u201d cake (single serving, about 100\u2013120 g): 350\u2013450 kcal\n     \u2022 Vanilla ice cream (one generous scoop, ~\u00bd cup): 130\u2013180 kcal\n     \u2022 Berry sauce or coulis (2\u20133 Tbsp): 30\u201360 kcal\n     \u2022 Fresh strawberry slices (3\u20134 medium berries): 10\u201315 kcal\n\n     Total estimate: approximately 520\u2013705 kcal\n\n     Depending on exact serving sizes, you\u2019re likely looking at somewhere in the 550\u2013750 kcal range for the entire plate\n</code></pre>"},{"location":"01-Workshop/02-Ideation/02/#7-homework-try-this","title":"7. Homework: Try this","text":"<p>EXPLORE AT HOME: Iterate and build intuition for yourself.</p> <p>Think of ways in which we can make the task more actionable for our needs.</p> <ol> <li>Response Formatting - can we have it generate the information in form of a nutrition label?</li> <li>Recipe Suggestion - can we have it generate a recipe just by looking at the picture?</li> <li>Visual Reimagining - can we have it create a new image for the recipe reflecting our style?</li> </ol> SAMPLE RESPONSE: \"Give me a recipe for this that matches the formats of online recipe websites. Provide ingredients prep-time cook-time and step by step instructions (Took 1108 tokens)\" <p>Molten Chocolate Lava Cakes with Berry Coulis and Fresh Strawberries</p> <p>Yields: 4 servings   Prep Time: 20 minutes   Cook Time: 12 minutes   Total Time: 32 minutes</p> <p>INGREDIENTS   For the Lava Cakes   \u2022 120 g (4 oz) dark chocolate (70% cocoa), chopped   \u2022 100 g (3\u00bd oz) unsalted butter, plus extra for greasing   \u2022 2 large eggs   \u2022 2 large egg yolks   \u2022 50 g (\u00bc cup) caster (superfine) sugar   \u2022 25 g (3 tbsp) plain (all\u2011purpose) flour   \u2022 Pinch of fine sea salt</p> <p>For the Berry Coulis   \u2022 150 g (5 oz) mixed berries (fresh or frozen; e.g. raspberries, blueberries, blackberries)   \u2022 2 tbsp granulated sugar   \u2022 1 tbsp fresh lemon juice   \u2022 1\u20132 tbsp water (as needed)</p> <p>To Serve   \u2022 4 scoops high\u2011quality vanilla ice cream   \u2022 4 large strawberries, hulled and sliced</p> <p>EQUIPMENT   \u2022 4 \u00d7 6 oz (180 ml) ramekins or custard cups   \u2022 Baking tray   \u2022 Small saucepan or double\u2011boiler setup   \u2022 Whisk, spatula, sieve, and fine\u2011mesh strainer</p> <p>STEP\u2011BY\u2011STEP INSTRUCTIONS</p> <p>Preheat &amp; prepare ramekins   \u2013 Preheat the oven to 200 \u00b0C (400 \u00b0F).   \u2013 Generously butter each ramekin, then dust with a little sugar, tapping out any excess. Set ramekins on a baking tray.</p> <p>Melt chocolate &amp; butter   \u2013 In a heatproof bowl set over (but not touching) simmering water, combine chopped dark chocolate and butter.   \u2013 Stir occasionally until fully melted and smooth. Remove from heat and let cool for 2 minutes.</p> <p>Whisk eggs &amp; sugar   \u2013 In a separate bowl, whisk together whole eggs, yolks, sugar, and salt until pale, thick, and ribbon\u2011coating (about 2\u20133 minutes).</p> <p>Combine chocolate mixture   \u2013 Gently fold the melted chocolate/butter into the egg mixture using a rubber spatula. Mix until just combined.</p> <p>Fold in flour   \u2013 Sift the flour over the batter, then fold in lightly until no streaks of flour remain. Be careful not to overmix.</p> <p>Fill ramekins &amp; bake   \u2013 Divide batter evenly among the prepared ramekins.   \u2013 Bake on the center rack for 10\u201312 minutes, until the edges are set but the centers still jiggle slightly when shaken.</p> <p>Rest briefly   \u2013 Remove from oven and let rest for 1 minute. Run a small knife around the edge of each cake, then invert onto individual serving plates.</p> <p>Make the berry coulis   \u2013 In a small saucepan, combine berries, sugar, and lemon juice. Cook over medium heat until the fruit breaks down and releases its juices (3\u20135 minutes).   \u2013 Add water by the tablespoon if mixture seems too thick.   \u2013 Strain through a fine\u2011mesh sieve into a bowl, pressing on solids to extract as much sauce as possible. Discard seeds.</p> <p>Plate &amp; garnish   \u2013 Using the back of a spoon, swoosh about 2\u20133 tbsp of coulis in a circle on each plate.   \u2013 Place the warm lava cake in the center of the coulis swirl.   \u2013 Top each cake with a scoop of vanilla ice cream.   \u2013 Arrange sliced strawberries alongside or on top.</p> <p>Serve immediately   \u2013 The contrast of warm, gooey chocolate and cold ice cream is best enjoyed right away.</p> <p>TIPS   \u2022 To make ahead: Assemble batter in ramekins, cover tightly, and refrigerate up to 24 hours. Add 1\u20132 extra minutes to baking time if chilled.   \u2022 For an extra flourish: Dust with icing sugar or sprinkle a few fresh mint leaves.</p>"},{"location":"01-Workshop/03-Inference/01/","title":"Lab 7 \u00b7 Code-First Prompting","text":"<p>BY THE END OF THIS SECTION YOU SHOULD HAVE</p> <ol> <li>Successfully run your first code-first reasoning prompt from Jupyter notebooks</li> <li>Understood the token counts and performance impacts of reasoning vs. general-purpose models</li> <li>Explored various text scenarios interactively to build intuition for code-first usage</li> <li>A \"sandbox\" notebook that you can quickly use to explore scenarios &amp; prompt engineering</li> </ol> <p>REMINDER: These models are stochastic. Your response may not match the sample exactly</p>"},{"location":"01-Workshop/03-Inference/01/#1-get-started-with-notebooks","title":"1. Get Started With Notebooks","text":"<p>In this section, we'll move from Playground (low-code) to SDK (code-first) and see how we can working with reasoning models directly from Jupyter Notebooks.</p> <p>HOW TO RUN A JUPYTER NOTEBOOK</p> <p>This codespaces is setup with the extensions and environment to make it easy.</p> <ol> <li>Click the notebook in the File Explorer \u2192 opens it in editor</li> <li>Click Select Kernel \u2192 pick the default Python environment</li> <li>Click Clear all Outputs \u2192 just in case prior run outputs existed</li> <li>Click Run all \u2192 this will execute all the exercises at one shot</li> </ol> <p>HOW TO USE A JUPYTER NOTEBOOK</p> <p>After a \"Run all\" you can interactively reviw the notebook cells:</p> <ul> <li>Read the markdown cell to know what that code does</li> <li>See the code cell to learn how that was implemented</li> <li>See the code output area to see execution responses</li> <li>Important: See the execution time per cell (intuitive response time).</li> </ul> <p>This lets you do two things:</p> <ol> <li>Ensure that the notebook executes first (and debug errors)</li> <li>Change any single cell (code) and re-run it (to build intution)</li> </ol> <p>Ready? Let's go!</p>"},{"location":"01-Workshop/03-Inference/01/#1-notebook-1-validate","title":"1. Notebook 1: Validate","text":"<p>OBJECTIVE: Validate development environment. Understand output format</p> <ol> <li>Open the VS Code editor to the <code>notebooks</code> folder.</li> <li>Click on <code>00-setup-aoai.ipynb</code> using instructions above.</li> <li>Verify all cells ran without error.</li> </ol> <p>Now, let's talk about what you learned in this notebook</p> <ol> <li>We tested an older reasoning model (o1)</li> <li>We tested a newer reasoning model (o4-mini)</li> <li>We tested a general-purpose model (gpt-4o-mini)</li> <li>We printed out the response and usage stats for each.</li> </ol> <p>EXAMPLE OUTPUT: Pay attention to response accuracy and token costs</p> gpt-4o-minio1o4-mini <p>How many r's in strawberry?</p> <pre><code>\ud83d\udde3\ufe0f gpt-4o-mini returned:\nThere are three 'r's in the word \"strawberry.\"\n.........................\nToken Costs:\nTotal Tokens: 29\nPrompt Tokens: 14\nCompletion Tokens: 15\nReasoning Tokens: 0\nOutput Tokens: 29\n.........................\nContent Filter Results:\nprotected_material_code: {'filtered': False, 'detected': False}\nprotected_material_text: {'filtered': False, 'detected': False}\n.........................\n</code></pre> <p>How many r's in strawberry?</p> <pre><code>\ud83d\udde3\ufe0f o1 returned:\nThere are 3 \u201cr\u201d letters in \u201cstrawberry.\u201d\n.........................\nToken Costs:\nTotal Tokens: 294\nPrompt Tokens: 13\nCompletion Tokens: 281\nReasoning Tokens: 256\nOutput Tokens: 38\n.........................\nContent Filter Results:\nhate: {'filtered': False, 'severity': 'safe'}\nprotected_material_code: {'filtered': False, 'detected': False}\nprotected_material_text: {'filtered': False, 'detected': False}\nself_harm: {'filtered': False, 'severity': 'safe'}\nsexual: {'filtered': False, 'severity': 'safe'}\nviolence: {'filtered': False, 'severity': 'safe'}\n.........................\n</code></pre> <p>How many r's in strawberry?</p> <pre><code>\ud83d\udde3\ufe0f o4-mini returned:\nThe word \u201cstrawberry\u201d has three \u201cr\u201d letters.\n.........................\nToken Costs:\nTotal Tokens: 304\nPrompt Tokens: 13\nCompletion Tokens: 291\nReasoning Tokens: 256\nOutput Tokens: 48\n.........................\nContent Filter Results:\nhate: {'filtered': False, 'severity': 'safe'}\nprotected_material_code: {'filtered': False, 'detected': False}\nprotected_material_text: {'filtered': False, 'detected': False}\nself_harm: {'filtered': False, 'severity': 'safe'}\nsexual: {'filtered': False, 'severity': 'safe'}\nviolence: {'filtered': False, 'severity': 'safe'}\n.........................\n</code></pre>"},{"location":"01-Workshop/03-Inference/01/#2-notebook-2-quickstart","title":"2. Notebook 2: Quickstart","text":"<p>OBJECTIVE: Explore different use cases. Build intuition for model Response, Token Costs</p> <ol> <li>Open the VS Code editor to the <code>notebooks</code> folder.</li> <li>Click on <code>00-text-reasoning.ipynb</code> using instructions above.</li> <li>Verify all cells ran without error.</li> </ol>"},{"location":"01-Workshop/03-Inference/01/#4-homework-build-intuition","title":"4. Homework: Build Intuition","text":"<p>OBJECTIVE: Put these learnings to use with these notebooks as your sandbox!</p> <ol> <li>Each notebook has a final cell with an area for you to try your own prompt</li> <li>Fill in a prompt - and run just that cell. Observe outcome</li> <li>Try changing parameters - observe impact on response and token costs</li> <li>Build intuition for a specific model or use case - by trying things out!</li> </ol> <p>Think of problems in areas like:</p> <ul> <li>Logic and deduction \u00b7 <code>A farmer has 17 sheep. All but 9 run away. How many are left?</code> \u00b7 This is about ability to interpret word problems, and not the math itself.</li> <li>Cause and effect \u00b7 <code>If the power goes out during a storm, what are three possible consequences in a modern household?</code> \u00b7 Tests causal reasoning and practical inference</li> <li>Pattern recognition \u00b7 <code>A dog is to bark as a cat is to __?</code> \u00b7 Tests analogy and associative reasoning</li> <li>Counterfactual or hypothetical thinking \u00b7 <code>Imagine a world where people need just 4 hours of sleep. How does this impact our work and school lives? Discuss the benefits and drawbacks and back this up with research.</code> \u00b7 Tests deep research.</li> </ul>"},{"location":"01-Workshop/03-Inference/02/","title":"Lab 8 \u00b7 Advanced Usage","text":"<p>BY THE END OF THIS SECTION YOU SHOULD HAVE</p> <ol> <li>Learned to explore more visual reasoning problems</li> <li>Learned to modify reasoning effort via the API call</li> <li>Leared to explore other capabilities with reasoning</li> </ol> <p>REMINDER: These models are stochastic. Your response may not match the sample exactly</p>"},{"location":"01-Workshop/03-Inference/02/#explore-advanced-use","title":"Explore Advanced Use","text":"<p>OBJECTIVE: Explore richer API capabilities. Build intuition usage in advanced scenarios</p> <ol> <li>Open the VS Code editor to the <code>notebooks</code> folder.</li> <li>Click on <code>02-advanced-reasoning.ipynb</code> using instructions above.</li> <li>Verify all cells ran without error.</li> </ol> <p>Now, let's talk about what you learned in this notebook</p> <ol> <li>We saw how you can use capabilities like Structured Outputs</li> <li>We saw how the API can be tailored for Developer Messages</li> <li>We saw additional use cases for Visual Reasoning</li> <li>We set the notebook up for rapid ideation using these features </li> </ol>"},{"location":"01-Workshop/03-Inference/02/#build-your-intuition","title":"Build Your Intuition","text":"<p>OBJECTIVE: Put these learnings to use with these notebooks as your sandbox!</p> <ol> <li>Each notebook has a final cell with an area for you to try your own prompt</li> <li>Fill in a prompt - and run just that cell. Observe outcome</li> <li>Try changing parameters - observe impact on response and token costs</li> <li>Build intuition for a specific model or use case - by trying things out!</li> </ol> <p>Think of problems in areas like:</p> <ol> <li>Location Recognition - \"Where is this?\" with a photo from a travel guide</li> <li>Decipher Old Books or Paintings - \"What does this say?\" with hieroglyphics</li> <li>Create a game - \"Code angry bird - but with emojis vs. bugs\"</li> <li>Debug an error - \"What is the issue here?\" - but with a screenshot of the error</li> <li>Solve a game - \"Can black win?\" - with a screenshot of a chess game in progress</li> </ol>"},{"location":"01-Workshop/03-Inference/03/","title":"Continue Learning","text":"<p>Congratulations! You just completed the Reasoning Lab333 at Microsoft Build</p> <p>You now have a repository you can continue to explore at home, with your own ideas. Here are a couple of suggestions to help you keep going!</p>"},{"location":"01-Workshop/03-Inference/03/#how-to-generate-ideas","title":"How to Generate Ideas","text":"<p>Still trying to understand what a good reasoning prompt could be? Try asking a general-purpose model to generate a few ideas for you. </p> <p>TRY THIS PROMPT YOURSELF</p> <p>Give me 10 examples of reasoning prompts that are good for o4-mini. Give me examples for math, science, multi-step planning and constraint-based scheduling</p> <p>Remember that these models are not perfect - assess the examples for yourself. Below are some of the examples it generated for me.</p>"},{"location":"01-Workshop/03-Inference/03/#how-to-evaluate-ideas","title":"How to Evaluate Ideas","text":"<p>HOMEWORK: Try these out in your notebook sandbox - then experiment with modifications!</p>"},{"location":"01-Workshop/03-Inference/03/#51-math-reasoning","title":"5.1 \u2797 Math + Reasoning","text":"<ol> <li> <p>\u201cA train travels at 60 mph. How long does it take to travel 90 miles?\u201d (Basic rate/time/distance reasoning.)</p> </li> <li> <p>\u201cJane has twice as many apples as Tom. Together they have 18 apples. How many does each person have?\u201d (Simple algebraic setup requiring translation from words to equations.)</p> </li> <li> <p>\u201cA rectangle has a perimeter of 36 cm. If the length is twice the width, what are the rectangle's dimensions?\u201d (Tests geometry and equation solving.)</p> </li> </ol>"},{"location":"01-Workshop/03-Inference/03/#52-science-reasoning","title":"5.2 \ud83d\udd2c Science + Reasoning","text":"<ol> <li> <p>\u201cWhy does a metal spoon feel colder than a wooden spoon when left in the same room?\u201d (Tests understanding of thermal conductivity and sensory perception.)</p> </li> <li> <p>\u201cIf you double the mass of an object, how does that affect the force needed to accelerate it at the same rate?\u201d (Applies Newton's Second Law: F = ma.)</p> </li> <li> <p>\u201cWhat would happen to a plant placed in a dark room for 10 days, and why?\u201d (Encourages explanation of photosynthesis and light dependency.)</p> </li> </ol>"},{"location":"01-Workshop/03-Inference/03/#53-multi-step-planning","title":"5.3 \ud83d\udccb Multi-Step + Planning","text":"<ol> <li> <p>\u201cYou need to plan a 3-day conference. List the major steps to take from now until the event, including choosing a venue, speakers, and marketing.\u201d (Tests ability to sequence actions with dependencies.)</p> </li> <li> <p>\u201cDesign a basic weekly study schedule to learn Python in 6 weeks, assuming 1 hour per weekday.\u201d (Combines time management, curriculum planning, and prioritization.)</p> </li> </ol>"},{"location":"01-Workshop/03-Inference/03/#54-constraint-scheduling","title":"5.4 \u23f3 Constraint + Scheduling","text":"<ol> <li> <p>\u201cYou have 4 tasks: A (2 hrs), B (1 hr), C (3 hrs), D (2 hrs). You have 5 hours available today. Task A must come before C. What\u2019s a valid schedule?\u201d (Tests ability to honor constraints and fit within a time window.)</p> </li> <li> <p>\u201cThree employees\u2014Alex, Sam, and Riley\u2014must each work one 4-hour shift today. Only Alex and Riley can work before noon, and Sam can\u2019t work past 4 PM. Create a valid schedule.\u201d (Combines logic, constraints, and scheduling.)</p> </li> </ol>"},{"location":"02-Homework/Implementation/","title":"Building a Reasoning Application","text":"<p>THIS IS NOT A LAB EXERCISE - IT IS A THOUGHT EXERCISE TO INSPIRE YOU</p> <p>This section is not meant to completed in-venue. </p> <p>It is meant for you to read and think about, in how reasoning models can be used to build more effective applications. This entire track was created by a reasoning model using the prompt below. The code or content has not been tested or validated but is shown exactly as generated, to give you a sense of how these models can be used to plan your next application</p> <pre><code>Build a Reasoning Model Application - apply prompt engineering best practices to a complex scenario. In this section, we want to build a complex catering application that, given a set of recipes, and a set of catering orders, will generate a catering plan that covers ingredient purchase, recipe adjustments (for servings), and timing requirements.\nTalk about how we can use the reasoning models to help us with this task.\n- Explain metaprompting \n- Explain orchestration (where reasoning model plans and gpt model executes)\n</code></pre> <p>THINK ABOUT THIS AS YOU READ</p> <p>Look at the content generated in this track</p> <ul> <li>How coherent is the writing?</li> <li>How comprehensive is the planning?</li> <li>How accurate are the explanations?</li> <li>How runnable is the code?</li> <li>How can using this as a starting point help productivity?</li> <li>What aspects of app development are good targets for reasoning?</li> </ul> <p>Find the idea interesting? Star the repo and watch for updates with more labs to build on this</p> <p>This section applies everything you've learned to build a practical application that leverages reasoning models. You'll develop a catering planning system that demonstrates how to effectively use reasoning capabilities to solve complex planning and optimization problems.</p>"},{"location":"02-Homework/Implementation/#learning-objectives","title":"Learning Objectives","text":"<p>After completing this section, you will be able to:</p> <ul> <li>Design applications that effectively leverage reasoning model capabilities</li> <li>Implement advanced prompting techniques specifically designed for reasoning models</li> <li>Orchestrate multiple models based on their strengths and cost profiles</li> <li>Handle complex planning problems with multiple constraints</li> <li>Build robust, production-ready solutions using Azure AI Foundry</li> </ul>"},{"location":"02-Homework/Implementation/#topics-covered","title":"Topics Covered","text":"<ol> <li>Project Overview - Understand the catering application requirements and architecture</li> <li>Metaprompting Techniques - Learn advanced prompting strategies for reasoning models</li> <li>Model Orchestration - Implement a system where reasoning models plan and GPT models execute</li> </ol>"},{"location":"02-Homework/Implementation/#application-components","title":"Application Components","text":"<p>The catering planning application includes several core components:</p> <ul> <li>Recipe Processor: Parses and adjusts recipes based on guest count</li> <li>Shopping List Generator: Consolidates ingredients across multiple recipes</li> <li>Timeline Creator: Plans preparation schedules with dependencies</li> <li>Resource Optimizer: Allocates kitchen resources efficiently</li> </ul>"},{"location":"02-Homework/Implementation/#approach","title":"Approach","text":"<p>This section takes a project-based approach, guiding you through the process of building a practical application. You'll implement core components, integrate reasoning models, and create a cohesive system that demonstrates the real-world value of reasoning capabilities.</p> <p>Begin with the first topic to start building your reasoning-powered application.</p>"},{"location":"02-Homework/Implementation/01/","title":"Project Overview: Catering Planning Application","text":"<p>In this section, we'll apply what we've learned about reasoning models to build a practical application: a sophisticated catering planning system. This project demonstrates how reasoning models can tackle complex problems with multiple constraints, planning requirements, and optimization needs.</p>"},{"location":"02-Homework/Implementation/01/#application-purpose","title":"Application Purpose","text":"<p>The Catering Planning Application helps catering businesses efficiently plan events by:</p> <ol> <li>Processing recipes and adjusting servings based on guest count</li> <li>Generating comprehensive shopping lists with consolidated ingredients</li> <li>Creating detailed preparation timelines with dependencies</li> <li>Optimizing resource allocation for multiple concurrent events</li> <li>Planning kitchen workflow to maximize efficiency</li> </ol> <p>This represents the type of complex planning challenge where reasoning models excel over traditional software approaches or simpler AI models.</p>"},{"location":"02-Homework/Implementation/01/#core-functionality","title":"Core Functionality","text":""},{"location":"02-Homework/Implementation/01/#recipe-processing","title":"Recipe Processing","text":"<p>The application needs to: - Parse recipe ingredient lists and instructions - Adjust quantities for different serving sizes - Identify common ingredients across multiple recipes - Account for inventory already on hand</p>"},{"location":"02-Homework/Implementation/01/#shopping-list-generation","title":"Shopping List Generation","text":"<p>The system will: - Consolidate ingredients across all recipes for an event - Convert between different units of measurement - Group ingredients by store department - Optimize purchasing by suggesting bulk buys when appropriate</p>"},{"location":"02-Homework/Implementation/01/#timeline-creation","title":"Timeline Creation","text":"<p>The application should: - Determine preparation order based on recipe dependencies - Calculate preparation and cooking times - Identify tasks that can be done in advance - Schedule activities to minimize wait times and ensure all dishes are ready simultaneously</p>"},{"location":"02-Homework/Implementation/01/#technical-architecture","title":"Technical Architecture","text":"<p>Our application will use a layered architecture:</p> <ol> <li>Data Layer: JSON files representing recipes, inventory, and orders</li> <li>Processing Layer: Python code for data manipulation and model orchestration</li> <li>AI Layer: Calls to Azure AI Foundry reasoning models for complex planning</li> <li>Output Layer: JSON and text outputs representing plans and schedules</li> </ol>"},{"location":"02-Homework/Implementation/01/#model-roles","title":"Model Roles","text":"<p>We'll leverage different models for specific tasks:</p> <ul> <li>Claude (o1): High-level planning and complex reasoning (with high reasoning level)</li> <li>Claude 3 Mini (o3-mini): Intermediate tasks like recipe adjustments (with medium reasoning level)</li> <li>GPT-4o Mini: Quick operations like unit conversions (with low/no explicit reasoning)</li> </ul> <p>This approach demonstrates effective orchestration of models based on their strengths.</p>"},{"location":"02-Homework/Implementation/01/#development-approach","title":"Development Approach","text":"<p>We'll build this application incrementally:</p> <ol> <li>First implement core recipe processing functionality</li> <li>Then add shopping list generation capabilities</li> <li>Integrate timeline planning functions</li> <li>Finally, connect components into a cohesive system</li> </ol> <p>Throughout development, we'll explore techniques for: - Prompt engineering specific to reasoning models - Effectively decomposing complex problems - Building robust error handling and validation - Optimizing performance and cost through model selection</p>"},{"location":"02-Homework/Implementation/01/#data-structures","title":"Data Structures","text":""},{"location":"02-Homework/Implementation/01/#recipe-format","title":"Recipe Format","text":"<p>Recipes will be stored as JSON objects with a structure similar to:</p> JSON<pre><code>{\n  \"name\": \"Classic Beef Lasagna\",\n  \"serves\": 8,\n  \"prep_time\": 45,\n  \"cook_time\": 60,\n  \"total_time\": 120,\n  \"ingredients\": [\n    {\"name\": \"ground beef\", \"quantity\": 2, \"unit\": \"lb\"},\n    {\"name\": \"lasagna noodles\", \"quantity\": 12, \"unit\": \"sheets\"},\n    // additional ingredients...\n  ],\n  \"instructions\": [\n    \"Brown the ground beef in a large skillet...\",\n    \"Layer the bottom of the baking dish with noodles...\",\n    // additional steps...\n  ],\n  \"equipment\": [\"large skillet\", \"baking dish\", \"mixing bowl\"]\n}\n</code></pre>"},{"location":"02-Homework/Implementation/01/#order-format","title":"Order Format","text":"<p>Catering orders will be defined as:</p> JSON<pre><code>{\n  \"event_id\": \"E12345\",\n  \"client\": \"Northwind Traders\",\n  \"date\": \"2025-05-15\",\n  \"guests\": 45,\n  \"meal_type\": \"dinner\",\n  \"recipes\": [\"Classic Beef Lasagna\", \"Caesar Salad\", \"Garlic Bread\"],\n  \"special_requirements\": \"Three vegetarian options needed\"\n}\n</code></pre>"},{"location":"02-Homework/Implementation/01/#success-criteria","title":"Success Criteria","text":"<p>By the end of this section, you will have built a catering planning application that:</p> <ol> <li>Successfully processes recipes and adjusts for different serving sizes</li> <li>Generates accurate, consolidated shopping lists</li> <li>Creates logical preparation timelines with dependencies</li> <li>Demonstrates effective use of reasoning models for complex planning tasks</li> <li>Shows how to orchestrate multiple models based on task requirements</li> </ol> <p>This practical application integrates all the concepts covered throughout the workshop and provides a template for building your own sophisticated reasoning-powered applications.</p> <p>In the next section, we'll explore advanced prompting techniques specifically designed for reasoning models.</p>"},{"location":"02-Homework/Implementation/02/","title":"Metaprompting Techniques for Reasoning Models","text":"<p>Reasoning models require specialized prompting techniques to achieve their full potential. In this section, we'll explore advanced metaprompting strategies specifically designed to enhance the reasoning capabilities of models like Claude and GPT-4o Mini.</p>"},{"location":"02-Homework/Implementation/02/#what-is-metaprompting","title":"What is Metaprompting?","text":"<p>Metaprompting involves creating prompts that guide not just what the model should answer, but how it should approach the reasoning process. These techniques:</p> <ul> <li>Establish explicit reasoning frameworks</li> <li>Define problem-solving methodologies  </li> <li>Create structured thinking patterns</li> <li>Enforce validation and self-checking</li> </ul>"},{"location":"02-Homework/Implementation/02/#core-metaprompting-techniques","title":"Core Metaprompting Techniques","text":""},{"location":"02-Homework/Implementation/02/#1-process-instructions","title":"1. Process Instructions","text":"<p>Process instructions tell the model how to approach a problem:</p> Text Only<pre><code>To solve this problem, please:\n1. First, identify all the variables and constraints\n2. Develop equations representing these constraints\n3. Solve the equations step by step\n4. Verify your solution by checking against the original constraints\n5. Provide your final answer\n</code></pre> <p>Benefits: - Creates a consistent problem-solving approach - Reduces errors by encouraging systematic thinking - Makes responses more predictable and structured</p>"},{"location":"02-Homework/Implementation/02/#2-framework-application","title":"2. Framework Application","text":"<p>Framework application guides the model to use specific analytical frameworks:</p> Text Only<pre><code>Analyze this business decision using the following frameworks:\n1. SWOT Analysis (Strengths, Weaknesses, Opportunities, Threats)\n2. Cost-Benefit Analysis with quantification where possible\n3. Risk Assessment identifying key uncertainties\n4. Implementation Timeline with major milestones\n\nFor each framework, show your reasoning and how it informs the final recommendation.\n</code></pre> <p>Benefits: - Ensures comprehensive analysis from multiple perspectives - Creates consistent structure across different problems - Leverages established thinking methodologies</p>"},{"location":"02-Homework/Implementation/02/#3-reasoning-chain-templates","title":"3. Reasoning Chain Templates","text":"<p>Reasoning chain templates enforce specific patterns of logical progression:</p> Text Only<pre><code>For this logical deduction problem:\n1. State what you know from the given information\n2. For each deductive step:\n   a. Identify what can be directly inferred\n   b. Explain why this inference is valid\n   c. Update your knowledge based on this inference\n3. Continue until you've reached a complete solution\n4. Verify no contradictions exist in your solution\n</code></pre> <p>Benefits: - Prevents logical leaps that skip important steps - Makes reasoning explicit and verifiable - Helps identify when reasoning breaks down</p>"},{"location":"02-Homework/Implementation/02/#4-self-review-instructions","title":"4. Self-Review Instructions","text":"<p>Self-review instructions prompt the model to critique its own work:</p> Text Only<pre><code>After providing your initial solution, please:\n1. Review your work for mathematical errors\n2. Check if any constraints were overlooked\n3. Identify potential alternative approaches\n4. Determine if your solution is optimal or merely satisfactory\n5. If needed, revise your solution based on this review\n</code></pre> <p>Benefits: - Catches errors the model might have made initially - Improves solution quality through iteration - Makes limitations and uncertainties explicit</p>"},{"location":"02-Homework/Implementation/02/#metaprompting-for-different-reasoning-types","title":"Metaprompting for Different Reasoning Types","text":""},{"location":"02-Homework/Implementation/02/#for-mathematical-reasoning","title":"For Mathematical Reasoning","text":"Text Only<pre><code>Approach this math problem using these steps:\n1. Define variables for all unknown quantities\n2. Translate the problem constraints into equations\n3. Determine which formulas or identities are relevant\n4. Show each algebraic manipulation step-by-step\n5. Verify dimensional consistency in your calculations\n6. Check your final answer with a numerical test case\n</code></pre>"},{"location":"02-Homework/Implementation/02/#for-logical-deduction","title":"For Logical Deduction","text":"Text Only<pre><code>Solve this logical puzzle by:\n1. Creating a table/matrix to track possibilities\n2. For each clue:\n   a. Identify what it definitively proves or disproves\n   b. Mark eliminated possibilities in your tracking system\n   c. Note any new deductions that can be made\n3. After each step, check which possibilities remain\n4. Continue until only one consistent solution remains\n</code></pre>"},{"location":"02-Homework/Implementation/02/#for-planning-problems","title":"For Planning Problems","text":"Text Only<pre><code>Develop a plan by:\n1. Identify all tasks that need to be completed\n2. Determine dependencies between tasks\n3. Estimate duration for each task\n4. Create a critical path diagram\n5. Allocate resources optimally\n6. Identify potential bottlenecks and contingencies\n7. Verify the plan satisfies all constraints\n</code></pre>"},{"location":"02-Homework/Implementation/02/#implementing-metaprompting-in-code","title":"Implementing Metaprompting in Code","text":"<p>When implementing metaprompting in our catering application code:</p> Python<pre><code>def generate_shopping_list(recipes, guests, inventory):\n    # Create a structured metaprompt\n    metaprompt = f\"\"\"\n    You are an expert catering planner. Create a consolidated shopping list for the following event:\n\n    Guests: {guests}\n    Recipes: {json.dumps(recipes, indent=2)}\n    Current Inventory: {json.dumps(inventory, indent=2)}\n\n    Follow this reasoning process:\n    1. For each recipe, calculate the adjusted quantities needed based on guest count\n    2. Identify all ingredients needed across all recipes\n    3. For ingredients that appear in multiple recipes, consolidate the quantities\n    4. Subtract any available inventory from required quantities\n    5. Convert all measurements to consistent units\n    6. Group ingredients by category (produce, meat, dairy, etc.)\n    7. Verify your calculations by double-checking total quantities\n\n    Format your response as a JSON object with categories as keys and ingredients as values.\n    For each ingredient, include name, quantity, and unit.\n    \"\"\"\n\n    # Call reasoning model with metaprompt\n    response = client.complete(\n        claude_sonnet_id,\n        {\n            \"prompt\": metaprompt,\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n            \"reasoning\": \"high\"  # Using high reasoning for complex planning\n        }\n    )\n\n    # Process and validate response\n    return json.loads(response.text)\n</code></pre>"},{"location":"02-Homework/Implementation/02/#catering-application-metaprompt-examples","title":"Catering Application Metaprompt Examples","text":"<p>For our catering application, we'll use these metaprompt templates:</p>"},{"location":"02-Homework/Implementation/02/#recipe-scaling-metaprompt","title":"Recipe Scaling Metaprompt","text":"Text Only<pre><code>You are a professional chef helping scale recipes for a catering business.\n\nGiven the original recipe that serves {original_servings} people:\n{recipe}\n\nPlease scale it to serve {target_servings} people by:\n1. Calculating the scaling factor (target \u00f7 original)\n2. Adjusting each ingredient quantity using this factor\n3. Rounding to practical measurements (e.g., no one measures 1.33 eggs)\n4. Noting any ingredients that don't scale linearly (e.g., spices, leavening agents)\n5. For any non-linear ingredients, providing appropriate adjustments\n6. Verifying the scaled recipe maintains proper proportions\n\nProvide the scaled recipe in JSON format matching the original structure.\n</code></pre>"},{"location":"02-Homework/Implementation/02/#timeline-planning-metaprompt","title":"Timeline Planning Metaprompt","text":"Text Only<pre><code>As an experienced catering operations manager, create a preparation timeline for the following event:\n{event_details}\n\nDevelop this timeline by:\n1. Analyzing each recipe's preparation and cooking time requirements\n2. Identifying tasks that must be done in sequence vs. those that can be parallel\n3. Working backward from service time to determine start times\n4. Accounting for limited resources (staff, equipment) when scheduling concurrent tasks\n5. Including buffer time for unexpected delays\n6. Marking critical path tasks that could impact overall timing\n7. Identifying prep work that can be completed 1-2 days in advance\n\nFormat your response as a JSON object with timed tasks in chronological order.\n</code></pre>"},{"location":"02-Homework/Implementation/02/#best-practices-for-reasoning-metaprompts","title":"Best Practices for Reasoning Metaprompts","text":"<ol> <li>Start Broad, Then Narrow: Begin with a general problem description, then specify the reasoning process</li> <li>Use Numbered Steps: Clearly enumerate the reasoning steps for clarity</li> <li>Set Reasoning Level Appropriately: Use higher reasoning levels for complex planning tasks</li> <li>Request Verification: Always include steps for the model to verify its work</li> <li>Specify Output Format: Clearly define how the response should be structured</li> <li>Include Relevant Context: Provide all information needed for reasoning, but avoid unnecessary details</li> <li>Balance Guidance and Flexibility: Provide enough structure without over-constraining the model</li> </ol> <p>In the next section, we'll explore how to orchestrate multiple models with different capabilities for our catering application.</p>"},{"location":"02-Homework/Implementation/03/","title":"Model Orchestration","text":"<p>This section explores strategies for effective orchestration of multiple AI models in a single application. We'll implement a hierarchical architecture where reasoning models handle complex planning while more efficient models execute simpler tasks.</p>"},{"location":"02-Homework/Implementation/03/#the-model-orchestration-concept","title":"The Model Orchestration Concept","text":"<p>Model orchestration involves:</p> <ol> <li>Routing tasks to the most appropriate model based on task complexity</li> <li>Coordinating multiple models working together on a single problem</li> <li>Managing information flow between models</li> <li>Optimizing for both performance and cost</li> </ol>"},{"location":"02-Homework/Implementation/03/#orchestration-patterns-for-reasoning-models","title":"Orchestration Patterns for Reasoning Models","text":""},{"location":"02-Homework/Implementation/03/#1-planning-execution-pattern","title":"1. Planning-Execution Pattern","text":"<p>In this pattern: - Reasoning Model (Planner): Develops the high-level strategy and breaks down complex problems - Efficient Model (Executor): Handles routine tasks following the plan</p> Python<pre><code>def process_catering_order(order):\n    # Step 1: Use reasoning model to create a plan\n    planning_prompt = f\"\"\"\n    As a catering operations expert, create a detailed plan for executing this order.\n    Break down the process into specific tasks, noting which require complex decision-making\n    versus which are routine calculations or transformations.\n\n    Order details:\n    {json.dumps(order, indent=2)}\n\n    Your plan should specify:\n    1. The sequence of major operations needed\n    2. Which operations require sophisticated reasoning\n    3. Which operations are straightforward calculations\n    4. What information needs to be passed between steps\n    5. Verification checks to ensure quality\n\n    Format your response as a JSON execution plan.\n    \"\"\"\n\n    # Use Claude (o1) with high reasoning for complex planning\n    plan_response = client.complete(\n        claude_sonnet_id,\n        {\n            \"prompt\": planning_prompt,\n            \"temperature\": 0.2,\n            \"max_tokens\": 2000,\n            \"reasoning\": \"high\"\n        }\n    )\n\n    execution_plan = json.loads(plan_response.text)\n\n    # Step 2: Execute the plan using appropriate models for each step\n    results = {}\n    for step in execution_plan[\"steps\"]:\n        if step[\"complexity\"] == \"high\":\n            # Use reasoning model for complex steps\n            results[step[\"id\"]] = execute_complex_step(step, order, results)\n        else:\n            # Use efficient model for simpler steps\n            results[step[\"id\"]] = execute_simple_step(step, order, results)\n\n    return combine_results(results)\n</code></pre>"},{"location":"02-Homework/Implementation/03/#2-hierarchical-decision-tree","title":"2. Hierarchical Decision Tree","text":"<p>This pattern creates a tree of models with different capabilities:</p> Text Only<pre><code>                   [Strategic Planner]\n                      Claude (o1)\n                     /            \\\n       [Resource Planner]      [Timeline Planner]\n       Claude 3 Mini           Claude 3 Mini\n        /         \\                /       \\\n[Ingredient    [Equipment      [Recipe   [Service\n Calculator]    Allocator]     Scheduler]  Coordinator]\n GPT-4o Mini    GPT-4o Mini    GPT-4o Mini GPT-4o Mini\n</code></pre> <p>Implementation example:</p> Python<pre><code>def hierarchical_planning(event):\n    # Level 1: Strategic planning with Claude (o1)\n    strategic_plan = strategic_planning_model(event)\n\n    # Level 2: Domain-specific planning with Claude 3 Mini\n    resource_plan = resource_planning_model(strategic_plan[\"resources\"])\n    timeline_plan = timeline_planning_model(strategic_plan[\"timeline\"])\n\n    # Level 3: Tactical execution with GPT-4o Mini\n    ingredients = ingredient_calculation_model(resource_plan[\"ingredients\"])\n    equipment = equipment_allocation_model(resource_plan[\"equipment\"])\n    schedule = recipe_scheduling_model(timeline_plan[\"recipes\"])\n    service = service_coordination_model(timeline_plan[\"service\"])\n\n    # Combine all outputs into final plan\n    return {\n        \"strategic_overview\": strategic_plan[\"overview\"],\n        \"resources\": {\n            \"ingredients\": ingredients,\n            \"equipment\": equipment\n        },\n        \"timeline\": {\n            \"preparation\": schedule,\n            \"service\": service\n        }\n    }\n</code></pre>"},{"location":"02-Homework/Implementation/03/#3-iterative-refinement-pattern","title":"3. Iterative Refinement Pattern","text":"<p>This pattern iteratively improves solutions:</p> Python<pre><code>def generate_catering_plan(order, max_iterations=3):\n    # Initial plan generation with reasoning model\n    current_plan = generate_initial_plan(order)\n\n    for i in range(max_iterations):\n        # Evaluate plan quality\n        evaluation = evaluate_plan(current_plan, order)\n\n        # Stop if plan meets quality threshold\n        if evaluation[\"quality_score\"] &gt; 0.9:\n            return current_plan\n\n        # Use reasoning model to create improvement suggestions\n        refinement_prompt = f\"\"\"\n        Review this catering plan and identify specific improvements:\n        {json.dumps(current_plan, indent=2)}\n\n        Order requirements:\n        {json.dumps(order, indent=2)}\n\n        Current evaluation:\n        {json.dumps(evaluation, indent=2)}\n\n        Suggest specific improvements to address these issues.\n        Prioritize the most critical problems first.\n        \"\"\"\n\n        refinement_response = client.complete(\n            claude_sonnet_id,\n            {\n                \"prompt\": refinement_prompt,\n                \"temperature\": 0.3,\n                \"max_tokens\": 1000,\n                \"reasoning\": \"high\"\n            }\n        )\n\n        # Update plan based on suggestions\n        current_plan = apply_improvements(current_plan, refinement_response.text)\n\n    return current_plan\n</code></pre>"},{"location":"02-Homework/Implementation/03/#model-selection-strategy","title":"Model Selection Strategy","text":"<p>For our catering application, we'll implement these model selection guidelines:</p> Task Type Recommended Model Reasoning Level Temperature Complex planning Claude (o1) High 0.1-0.3 Constraint solving Claude (o1) Medium-High 0.1-0.2 Recipe scaling Claude 3 Mini Medium 0.2-0.4 Timeline creation Claude (o1) Medium 0.1-0.3 Ingredient consolidation Claude 3 Mini Low-Medium 0.2-0.4 Unit conversion GPT-4o Mini N/A 0.0-0.1 Text formatting GPT-4o Mini N/A 0.3-0.5"},{"location":"02-Homework/Implementation/03/#implementing-model-orchestration","title":"Implementing Model Orchestration","text":"<p>Here's a practical implementation for our catering application:</p> Python<pre><code>class ModelOrchestrator:\n    def __init__(self, client):\n        self.client = client\n\n        # Define model IDs\n        self.planner_model = \"anthropic-claude-3-sonnet-20240229-v1:0\"  # Claude (o1)\n        self.mid_tier_model = \"anthropic-claude-3-haiku-20240307-v1:0\"  # Claude 3 Mini\n        self.efficient_model = \"gpt-4o-mini-2024-07-18\"                # GPT-4o Mini\n\n    def route_task(self, task_type, data, context=None):\n        \"\"\"Route a task to the appropriate model based on task type\"\"\"\n\n        task_routing = {\n            \"strategic_planning\": (self.planner_model, \"high\", 0.2),\n            \"constraint_solving\": (self.planner_model, \"high\", 0.1),\n            \"recipe_scaling\": (self.mid_tier_model, \"medium\", 0.3),\n            \"timeline_creation\": (self.planner_model, \"medium\", 0.2),\n            \"ingredient_consolidation\": (self.mid_tier_model, \"medium\", 0.3),\n            \"unit_conversion\": (self.efficient_model, None, 0.0),\n            \"text_formatting\": (self.efficient_model, None, 0.4)\n        }\n\n        if task_type not in task_routing:\n            raise ValueError(f\"Unknown task type: {task_type}\")\n\n        model_id, reasoning_level, temperature = task_routing[task_type]\n\n        # Get the task-specific prompt template\n        prompt = self.get_prompt_template(task_type, data, context)\n\n        # Prepare request\n        request = {\n            \"prompt\": prompt,\n            \"temperature\": temperature,\n            \"max_tokens\": 2000\n        }\n\n        # Add reasoning level if applicable\n        if reasoning_level and model_id.startswith(\"anthropic\"):\n            request[\"reasoning\"] = reasoning_level\n\n        # Call the model\n        response = self.client.complete(model_id, request)\n\n        # Process the response based on task type\n        return self.process_response(task_type, response.text)\n\n    def get_prompt_template(self, task_type, data, context):\n        \"\"\"Get the appropriate prompt template for the task type\"\"\"\n        # Implementation of task-specific prompt templates\n        pass\n\n    def process_response(self, task_type, response_text):\n        \"\"\"Process the model response based on task type\"\"\"\n        # Implementation of task-specific response processing\n        pass\n</code></pre>"},{"location":"02-Homework/Implementation/03/#error-handling-in-model-orchestration","title":"Error Handling in Model Orchestration","text":"<p>Robust model orchestration requires comprehensive error handling:</p> Python<pre><code>def execute_with_fallback(orchestrator, primary_task, fallback_task, data, context=None):\n    \"\"\"Execute a task with fallback to a different model if it fails\"\"\"\n    try:\n        result = orchestrator.route_task(primary_task, data, context)\n\n        # Validate result\n        is_valid, issues = validate_result(primary_task, result)\n        if is_valid:\n            return result\n\n        # If validation fails, log issues and try fallback\n        logger.warning(f\"Primary task {primary_task} produced invalid result: {issues}\")\n        result = orchestrator.route_task(fallback_task, data, context)\n\n        # Validate fallback result\n        is_valid, issues = validate_result(fallback_task, result)\n        if is_valid:\n            return result\n\n        # If fallback also fails, raise exception\n        raise ValueError(f\"Both primary and fallback tasks failed: {issues}\")\n\n    except Exception as e:\n        logger.error(f\"Error executing {primary_task}: {str(e)}\")\n        # Try fallback task if primary fails\n        return orchestrator.route_task(fallback_task, data, context)\n</code></pre>"},{"location":"02-Homework/Implementation/03/#performance-and-cost-optimization","title":"Performance and Cost Optimization","text":"<p>To optimize the performance and cost of our orchestrated system:</p> <ol> <li>Cache Common Operations: Store results of frequent operations</li> <li>Batch Similar Requests: Combine similar tasks to reduce API calls</li> <li>Progressive Detail: Start with rough plans and add detail only when needed</li> <li>Asynchronous Processing: Use async calls for independent operations</li> <li>Resource-Aware Routing: Consider current system load when routing tasks</li> </ol> Python<pre><code># Example of asynchronous model orchestration\nasync def process_recipes_async(orchestrator, recipes, guests):\n    # Process multiple recipes in parallel\n    tasks = []\n    for recipe in recipes:\n        task = asyncio.create_task(\n            orchestrator.route_task(\"recipe_scaling\", recipe, {\"guests\": guests})\n        )\n        tasks.append(task)\n\n    # Wait for all tasks to complete\n    scaled_recipes = await asyncio.gather(*tasks)\n    return scaled_recipes\n</code></pre>"},{"location":"02-Homework/Implementation/03/#next-steps","title":"Next Steps","text":"<p>In the next section, we'll implement the Recipe Processing component of our catering application, applying these orchestration patterns to handle recipe data and serving adjustments efficiently.</p>"},{"location":"02-Homework/References/","title":"References","text":""},{"location":"02-Homework/References/#openai","title":"OpenAI","text":"<ol> <li>Thinking with images \u00b7 Apr 2025</li> <li>How to use the Usage API and Cost API to monitor your OpenAI usage \u00b7 Jan 2025</li> <li>Reasoning over Code Quality and Security in GitHub Pull Requests \u00b7 Dec 2024</li> <li>Reasoning with o1 (Short Course) \u00b7 Dec 2024</li> <li>Enhance your prompts with meta prompting \u00b7 Oct 2024</li> <li>Using chained calls for o1 structured outputs \u00b7 Sep 2024</li> <li>Using reasoning for data validation \u00b7 Sep 2024</li> <li>Using reasoning for routine generation \u00b7 Sep 2024</li> <li>OpenAI o1-mini \u00b7 Sep 2024</li> <li>Learning to reason with LLMs \u00b7 Sep 2024</li> <li>GPT-4o mini: advancing cost-efficient intelligence \u00b7 Jul 2024</li> </ol>"},{"location":"02-Homework/References/#microsoft","title":"Microsoft","text":"<ol> <li>Introducing MAI-DS-R1 \u00b7 Apr 2025</li> <li>Azure OpenAI reasoning models \u00b7 Apr 2025</li> <li>General-Purpose vs Reasoning Models in Azure OpenAI \u00b7 Apr 2025</li> <li>Get started with DeepSeek-R1 reasoning model in Azure AI model inference \u00b7 Apr 2025</li> <li>Prompt Engineering for OpenAI\u2019s O1 and O3-mini Reasoning Models \u00b7 Feb 2025</li> <li>Azure AI Model Inference Documentation \u00b7 Apr 2025</li> <li>Create and configure all the resources for Azure AI model inference \u00b7 Apr 2025</li> <li>Upgrade from GitHub Models to Azure AI model inference \u00b7 Apr 2025</li> <li>Configure your AI project to use Azure AI model inference \u00b7 Apr 2025</li> </ol>"},{"location":"02-Homework/References/#community","title":"Community","text":"<ol> <li>Understanding Reasoning LLMs \u00b7 Feb 2025</li> </ol>"}]}